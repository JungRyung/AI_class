{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정\n",
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breast_cancer dataset laod\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터셋 테스트데이터셋 (8:2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,415\n",
      "Trainable params: 1,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "model = Sequential([\n",
    "    Dense(30, input_dim=30, activation='relu'),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "import os\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 조건 설정\n",
    "modelpath = MODEL_DIR + \"final{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "checkpointer_callback = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 134.33990, saving model to ./model/final001-134.3399.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 134.33990 to 127.64584, saving model to ./model/final002-127.6458.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 127.64584 to 121.05027, saving model to ./model/final003-121.0503.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 121.05027 to 114.54957, saving model to ./model/final004-114.5496.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 114.54957 to 108.13517, saving model to ./model/final005-108.1352.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 108.13517 to 101.80666, saving model to ./model/final006-101.8067.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 101.80666 to 95.57147, saving model to ./model/final007-95.5715.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 95.57147 to 89.53135, saving model to ./model/final008-89.5313.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 89.53135 to 83.76739, saving model to ./model/final009-83.7674.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 83.76739 to 78.28297, saving model to ./model/final010-78.2830.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 78.28297 to 73.16546, saving model to ./model/final011-73.1655.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 73.16546 to 68.26634, saving model to ./model/final012-68.2663.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.26634 to 63.46494, saving model to ./model/final013-63.4649.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 63.46494 to 58.76172, saving model to ./model/final014-58.7617.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 58.76172 to 54.10391, saving model to ./model/final015-54.1039.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 54.10391 to 49.49268, saving model to ./model/final016-49.4927.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.49268 to 44.99257, saving model to ./model/final017-44.9926.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.99257 to 40.61995, saving model to ./model/final018-40.6199.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.61995 to 36.38651, saving model to ./model/final019-36.3865.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.38651 to 32.29057, saving model to ./model/final020-32.2906.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.29057 to 28.22865, saving model to ./model/final021-28.2287.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.22865 to 24.07057, saving model to ./model/final022-24.0706.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.07057 to 19.88191, saving model to ./model/final023-19.8819.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 19.88191 to 15.69100, saving model to ./model/final024-15.6910.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 15.69100 to 11.53544, saving model to ./model/final025-11.5354.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 11.53544 to 7.86467, saving model to ./model/final026-7.8647.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 7.86467 to 5.57590, saving model to ./model/final027-5.5759.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 5.57590 to 4.35870, saving model to ./model/final028-4.3587.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 4.35870 to 3.43463, saving model to ./model/final029-3.4346.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.43463 to 2.69639, saving model to ./model/final030-2.6964.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.69639 to 2.29930, saving model to ./model/final031-2.2993.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.29930 to 2.22178, saving model to ./model/final032-2.2218.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.22178 to 2.16552, saving model to ./model/final033-2.1655.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.16552 to 2.08783, saving model to ./model/final034-2.0878.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.08783 to 1.99123, saving model to ./model/final035-1.9912.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.99123 to 1.87835, saving model to ./model/final036-1.8783.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.87835 to 1.75213, saving model to ./model/final037-1.7521.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.75213 to 1.61612, saving model to ./model/final038-1.6161.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.61612 to 1.47494, saving model to ./model/final039-1.4749.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.47494 to 1.33507, saving model to ./model/final040-1.3351.hdf5\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.33507 to 1.20637, saving model to ./model/final041-1.2064.hdf5\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.20637 to 1.10314, saving model to ./model/final042-1.1031.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.10314 to 1.03834, saving model to ./model/final043-1.0383.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.03834 to 1.01010, saving model to ./model/final044-1.0101.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.01010 to 1.00581, saving model to ./model/final045-1.0058.hdf5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.00581\n",
      "\n",
      "Epoch 00068: val_loss improved from 1.00581 to 0.98629, saving model to ./model/final068-0.9863.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.98629 to 0.96992, saving model to ./model/final069-0.9699.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.96992 to 0.95766, saving model to ./model/final070-0.9577.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.95766 to 0.94853, saving model to ./model/final071-0.9485.hdf5\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.94853 to 0.94163, saving model to ./model/final072-0.9416.hdf5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.94163 to 0.93629, saving model to ./model/final073-0.9363.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.93629 to 0.93231, saving model to ./model/final074-0.9323.hdf5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.93231 to 0.92958, saving model to ./model/final075-0.9296.hdf5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.92958 to 0.92799, saving model to ./model/final076-0.9280.hdf5\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.92799\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.92799 to 0.92369, saving model to ./model/final084-0.9237.hdf5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.92369 to 0.91538, saving model to ./model/final085-0.9154.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.91538 to 0.90520, saving model to ./model/final086-0.9052.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.90520 to 0.89375, saving model to ./model/final087-0.8938.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.89375 to 0.88162, saving model to ./model/final088-0.8816.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.88162 to 0.86959, saving model to ./model/final089-0.8696.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.86959 to 0.85790, saving model to ./model/final090-0.8579.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.85790 to 0.84715, saving model to ./model/final091-0.8472.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.84715 to 0.83759, saving model to ./model/final092-0.8376.hdf5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.83759 to 0.82900, saving model to ./model/final093-0.8290.hdf5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.82900 to 0.82134, saving model to ./model/final094-0.8213.hdf5\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.82134 to 0.81465, saving model to ./model/final095-0.8146.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.81465 to 0.80891, saving model to ./model/final096-0.8089.hdf5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.80891 to 0.80409, saving model to ./model/final097-0.8041.hdf5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.80409 to 0.80023, saving model to ./model/final098-0.8002.hdf5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00099: val_loss improved from 0.80023 to 0.79725, saving model to ./model/final099-0.7972.hdf5\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.79725 to 0.79477, saving model to ./model/final100-0.7948.hdf5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.79477 to 0.79274, saving model to ./model/final101-0.7927.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.79274 to 0.79093, saving model to ./model/final102-0.7909.hdf5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.79093 to 0.78908, saving model to ./model/final103-0.7891.hdf5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.78908 to 0.78686, saving model to ./model/final104-0.7869.hdf5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.78686 to 0.78413, saving model to ./model/final105-0.7841.hdf5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.78413 to 0.78082, saving model to ./model/final106-0.7808.hdf5\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.78082 to 0.77685, saving model to ./model/final107-0.7768.hdf5\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.77685 to 0.77237, saving model to ./model/final108-0.7724.hdf5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.77237 to 0.76757, saving model to ./model/final109-0.7676.hdf5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.76757 to 0.76265, saving model to ./model/final110-0.7626.hdf5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.76265 to 0.75770, saving model to ./model/final111-0.7577.hdf5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.75770 to 0.75289, saving model to ./model/final112-0.7529.hdf5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.75289 to 0.74831, saving model to ./model/final113-0.7483.hdf5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.74831 to 0.74407, saving model to ./model/final114-0.7441.hdf5\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.74407 to 0.74020, saving model to ./model/final115-0.7402.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.74020 to 0.73671, saving model to ./model/final116-0.7367.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.73671 to 0.73375, saving model to ./model/final117-0.7338.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.73375 to 0.73130, saving model to ./model/final118-0.7313.hdf5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.73130 to 0.72889, saving model to ./model/final119-0.7289.hdf5\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.72889 to 0.72665, saving model to ./model/final120-0.7267.hdf5\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.72665 to 0.72448, saving model to ./model/final121-0.7245.hdf5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.72448 to 0.72048, saving model to ./model/final122-0.7205.hdf5\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.72048 to 0.71523, saving model to ./model/final123-0.7152.hdf5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.71523 to 0.71041, saving model to ./model/final124-0.7104.hdf5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.71041 to 0.70424, saving model to ./model/final125-0.7042.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.70424 to 0.69405, saving model to ./model/final126-0.6940.hdf5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.69405 to 0.68473, saving model to ./model/final127-0.6847.hdf5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.68473 to 0.67622, saving model to ./model/final128-0.6762.hdf5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.67622 to 0.66991, saving model to ./model/final129-0.6699.hdf5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.66991 to 0.66241, saving model to ./model/final130-0.6624.hdf5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.66241 to 0.65213, saving model to ./model/final131-0.6521.hdf5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.65213 to 0.63689, saving model to ./model/final132-0.6369.hdf5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.63689 to 0.61822, saving model to ./model/final133-0.6182.hdf5\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.61822 to 0.59835, saving model to ./model/final134-0.5984.hdf5\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.59835 to 0.57998, saving model to ./model/final135-0.5800.hdf5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.57998 to 0.56485, saving model to ./model/final136-0.5648.hdf5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.56485 to 0.55330, saving model to ./model/final137-0.5533.hdf5\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.55330 to 0.54510, saving model to ./model/final138-0.5451.hdf5\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.54510 to 0.54044, saving model to ./model/final139-0.5404.hdf5\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.54044 to 0.53601, saving model to ./model/final140-0.5360.hdf5\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.53601 to 0.53485, saving model to ./model/final141-0.5349.hdf5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.53485\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.53485\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.53485\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.53485\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.53485 to 0.52564, saving model to ./model/final146-0.5256.hdf5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.52564 to 0.51351, saving model to ./model/final147-0.5135.hdf5\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.51351 to 0.50135, saving model to ./model/final148-0.5014.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.50135 to 0.49087, saving model to ./model/final149-0.4909.hdf5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.49087 to 0.48318, saving model to ./model/final150-0.4832.hdf5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.48318 to 0.47822, saving model to ./model/final151-0.4782.hdf5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.47822 to 0.47572, saving model to ./model/final152-0.4757.hdf5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.47572 to 0.47544, saving model to ./model/final153-0.4754.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.47544 to 0.47152, saving model to ./model/final159-0.4715.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.47152 to 0.46421, saving model to ./model/final160-0.4642.hdf5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.46421 to 0.45615, saving model to ./model/final161-0.4561.hdf5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.45615 to 0.44829, saving model to ./model/final162-0.4483.hdf5\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.44829 to 0.44142, saving model to ./model/final163-0.4414.hdf5\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.44142 to 0.43616, saving model to ./model/final164-0.4362.hdf5\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.43616 to 0.43249, saving model to ./model/final165-0.4325.hdf5\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.43249 to 0.43018, saving model to ./model/final166-0.4302.hdf5\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.43018 to 0.42891, saving model to ./model/final167-0.4289.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.42891 to 0.42780, saving model to ./model/final168-0.4278.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.42780 to 0.42585, saving model to ./model/final169-0.4258.hdf5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.42585 to 0.42219, saving model to ./model/final170-0.4222.hdf5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.42219 to 0.41692, saving model to ./model/final171-0.4169.hdf5\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.41692 to 0.41082, saving model to ./model/final172-0.4108.hdf5\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.41082 to 0.40459, saving model to ./model/final173-0.4046.hdf5\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.40459 to 0.39861, saving model to ./model/final174-0.3986.hdf5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.39861 to 0.39318, saving model to ./model/final175-0.3932.hdf5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.39318 to 0.38849, saving model to ./model/final176-0.3885.hdf5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.38849 to 0.38467, saving model to ./model/final177-0.3847.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.38467 to 0.38165, saving model to ./model/final178-0.3816.hdf5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.38165 to 0.37887, saving model to ./model/final179-0.3789.hdf5\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.37887 to 0.37593, saving model to ./model/final180-0.3759.hdf5\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.37593 to 0.37280, saving model to ./model/final181-0.3728.hdf5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.37280 to 0.36920, saving model to ./model/final182-0.3692.hdf5\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.36920 to 0.36525, saving model to ./model/final183-0.3653.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00184: val_loss improved from 0.36525 to 0.36099, saving model to ./model/final184-0.3610.hdf5\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.36099 to 0.35701, saving model to ./model/final185-0.3570.hdf5\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.35701 to 0.35358, saving model to ./model/final186-0.3536.hdf5\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.35358 to 0.35062, saving model to ./model/final187-0.3506.hdf5\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.35062 to 0.34815, saving model to ./model/final188-0.3481.hdf5\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.34815 to 0.34591, saving model to ./model/final189-0.3459.hdf5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.34591 to 0.34351, saving model to ./model/final190-0.3435.hdf5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.34351 to 0.34081, saving model to ./model/final191-0.3408.hdf5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.34081 to 0.33779, saving model to ./model/final192-0.3378.hdf5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.33779 to 0.33450, saving model to ./model/final193-0.3345.hdf5\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.33450 to 0.33107, saving model to ./model/final194-0.3311.hdf5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.33107 to 0.32771, saving model to ./model/final195-0.3277.hdf5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.32771 to 0.32464, saving model to ./model/final196-0.3246.hdf5\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.32464 to 0.32222, saving model to ./model/final197-0.3222.hdf5\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.32222 to 0.32036, saving model to ./model/final198-0.3204.hdf5\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.32036 to 0.31886, saving model to ./model/final199-0.3189.hdf5\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.31886 to 0.31746, saving model to ./model/final200-0.3175.hdf5\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.31746 to 0.31618, saving model to ./model/final201-0.3162.hdf5\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.31618 to 0.31476, saving model to ./model/final202-0.3148.hdf5\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.31476 to 0.31305, saving model to ./model/final203-0.3130.hdf5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.31305 to 0.31103, saving model to ./model/final204-0.3110.hdf5\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.31103 to 0.30881, saving model to ./model/final205-0.3088.hdf5\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.30881 to 0.30657, saving model to ./model/final206-0.3066.hdf5\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.30657 to 0.30447, saving model to ./model/final207-0.3045.hdf5\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.30447 to 0.30252, saving model to ./model/final208-0.3025.hdf5\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.30252 to 0.30081, saving model to ./model/final209-0.3008.hdf5\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.30081 to 0.29942, saving model to ./model/final210-0.2994.hdf5\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.29942 to 0.29823, saving model to ./model/final211-0.2982.hdf5\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.29823 to 0.29745, saving model to ./model/final212-0.2974.hdf5\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.29745 to 0.29708, saving model to ./model/final213-0.2971.hdf5\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.29708 to 0.29656, saving model to ./model/final214-0.2966.hdf5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.29656 to 0.29626, saving model to ./model/final215-0.2963.hdf5\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.29626 to 0.29600, saving model to ./model/final216-0.2960.hdf5\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.29600 to 0.29577, saving model to ./model/final217-0.2958.hdf5\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.29577 to 0.29568, saving model to ./model/final218-0.2957.hdf5\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.29568\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.29568 to 0.29367, saving model to ./model/final227-0.2937.hdf5\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.29367 to 0.29041, saving model to ./model/final228-0.2904.hdf5\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.29041 to 0.28802, saving model to ./model/final229-0.2880.hdf5\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.28802 to 0.28695, saving model to ./model/final230-0.2870.hdf5\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.28695 to 0.28670, saving model to ./model/final231-0.2867.hdf5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.28670\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.28670\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.28670\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.28670\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.28670 to 0.28600, saving model to ./model/final236-0.2860.hdf5\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.28600 to 0.28465, saving model to ./model/final237-0.2846.hdf5\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.28465 to 0.28400, saving model to ./model/final238-0.2840.hdf5\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.28400\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.28400\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.28400\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.28400\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.28400 to 0.28358, saving model to ./model/final243-0.2836.hdf5\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.28358 to 0.28266, saving model to ./model/final244-0.2827.hdf5\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.28266 to 0.28157, saving model to ./model/final245-0.2816.hdf5\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.28157 to 0.28051, saving model to ./model/final246-0.2805.hdf5\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.28051 to 0.27963, saving model to ./model/final247-0.2796.hdf5\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.27963 to 0.27924, saving model to ./model/final248-0.2792.hdf5\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.27924\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.27924\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.27924\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.27924\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.27924\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.27924 to 0.27890, saving model to ./model/final254-0.2789.hdf5\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.27890 to 0.27749, saving model to ./model/final255-0.2775.hdf5\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.27749 to 0.27683, saving model to ./model/final256-0.2768.hdf5\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.27683\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.27683\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.27683\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.27683\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.27683 to 0.27607, saving model to ./model/final261-0.2761.hdf5\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.27607 to 0.27500, saving model to ./model/final262-0.2750.hdf5\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.27500 to 0.27374, saving model to ./model/final263-0.2737.hdf5\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.27374 to 0.27253, saving model to ./model/final264-0.2725.hdf5\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.27253 to 0.27159, saving model to ./model/final265-0.2716.hdf5\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.27159 to 0.27145, saving model to ./model/final266-0.2714.hdf5\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.27145\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.27145\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.27145\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.27145 to 0.27075, saving model to ./model/final270-0.2707.hdf5\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.27075 to 0.26934, saving model to ./model/final271-0.2693.hdf5\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.26934 to 0.26809, saving model to ./model/final272-0.2681.hdf5\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.26809 to 0.26768, saving model to ./model/final273-0.2677.hdf5\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.26768\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.26768\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.26768\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.26768 to 0.26669, saving model to ./model/final277-0.2667.hdf5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.26669 to 0.26546, saving model to ./model/final278-0.2655.hdf5\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.26546 to 0.26447, saving model to ./model/final279-0.2645.hdf5\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.26447 to 0.26392, saving model to ./model/final280-0.2639.hdf5\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.26392 to 0.26372, saving model to ./model/final281-0.2637.hdf5\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.26372\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00283: val_loss improved from 0.26372 to 0.26321, saving model to ./model/final283-0.2632.hdf5\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.26321 to 0.26212, saving model to ./model/final284-0.2621.hdf5\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.26212 to 0.26120, saving model to ./model/final285-0.2612.hdf5\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.26120 to 0.26073, saving model to ./model/final286-0.2607.hdf5\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.26073 to 0.26067, saving model to ./model/final287-0.2607.hdf5\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.26067\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.26067 to 0.26054, saving model to ./model/final289-0.2605.hdf5\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.26054 to 0.26020, saving model to ./model/final290-0.2602.hdf5\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.26020 to 0.25983, saving model to ./model/final291-0.2598.hdf5\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.25983 to 0.25941, saving model to ./model/final292-0.2594.hdf5\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.25941 to 0.25896, saving model to ./model/final293-0.2590.hdf5\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.25896 to 0.25850, saving model to ./model/final294-0.2585.hdf5\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.25850 to 0.25799, saving model to ./model/final295-0.2580.hdf5\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.25799 to 0.25752, saving model to ./model/final296-0.2575.hdf5\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.25752 to 0.25710, saving model to ./model/final297-0.2571.hdf5\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.25710 to 0.25673, saving model to ./model/final298-0.2567.hdf5\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.25673 to 0.25640, saving model to ./model/final299-0.2564.hdf5\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.25640 to 0.25611, saving model to ./model/final300-0.2561.hdf5\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.25611 to 0.25606, saving model to ./model/final301-0.2561.hdf5\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.25606\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.25606 to 0.25572, saving model to ./model/final303-0.2557.hdf5\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.25572 to 0.25528, saving model to ./model/final304-0.2553.hdf5\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.25528 to 0.25478, saving model to ./model/final305-0.2548.hdf5\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.25478 to 0.25430, saving model to ./model/final306-0.2543.hdf5\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.25430 to 0.25389, saving model to ./model/final307-0.2539.hdf5\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.25389 to 0.25360, saving model to ./model/final308-0.2536.hdf5\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.25360 to 0.25340, saving model to ./model/final309-0.2534.hdf5\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.25340 to 0.25324, saving model to ./model/final310-0.2532.hdf5\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.25324 to 0.25298, saving model to ./model/final311-0.2530.hdf5\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.25298 to 0.25261, saving model to ./model/final312-0.2526.hdf5\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.25261 to 0.25217, saving model to ./model/final313-0.2522.hdf5\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.25217 to 0.25173, saving model to ./model/final314-0.2517.hdf5\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.25173 to 0.25135, saving model to ./model/final315-0.2513.hdf5\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.25135 to 0.25109, saving model to ./model/final316-0.2511.hdf5\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.25109 to 0.25093, saving model to ./model/final317-0.2509.hdf5\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.25093 to 0.25083, saving model to ./model/final318-0.2508.hdf5\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.25083 to 0.25068, saving model to ./model/final319-0.2507.hdf5\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.25068 to 0.25043, saving model to ./model/final320-0.2504.hdf5\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.25043 to 0.25009, saving model to ./model/final321-0.2501.hdf5\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.25009 to 0.24971, saving model to ./model/final322-0.2497.hdf5\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.24971 to 0.24936, saving model to ./model/final323-0.2494.hdf5\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.24936 to 0.24907, saving model to ./model/final324-0.2491.hdf5\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.24907\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.24907\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.24907\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.24907\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.24907\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.24907 to 0.24867, saving model to ./model/final330-0.2487.hdf5\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.24867 to 0.24809, saving model to ./model/final331-0.2481.hdf5\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.24809 to 0.24762, saving model to ./model/final332-0.2476.hdf5\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.24762 to 0.24732, saving model to ./model/final333-0.2473.hdf5\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.24732 to 0.24719, saving model to ./model/final334-0.2472.hdf5\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.24719 to 0.24716, saving model to ./model/final335-0.2472.hdf5\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.24716 to 0.24695, saving model to ./model/final336-0.2469.hdf5\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.24695 to 0.24664, saving model to ./model/final337-0.2466.hdf5\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.24664 to 0.24627, saving model to ./model/final338-0.2463.hdf5\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.24627 to 0.24586, saving model to ./model/final339-0.2459.hdf5\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.24586 to 0.24570, saving model to ./model/final340-0.2457.hdf5\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.24570\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.24570\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.24570\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.24570\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.24570\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.24570 to 0.24534, saving model to ./model/final346-0.2453.hdf5\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.24534 to 0.24494, saving model to ./model/final347-0.2449.hdf5\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.24494 to 0.24461, saving model to ./model/final348-0.2446.hdf5\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.24461 to 0.24440, saving model to ./model/final349-0.2444.hdf5\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.24440 to 0.24434, saving model to ./model/final350-0.2443.hdf5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.24434\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.24434\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.24434\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.24434 to 0.24418, saving model to ./model/final354-0.2442.hdf5\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.24418 to 0.24387, saving model to ./model/final355-0.2439.hdf5\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.24387 to 0.24350, saving model to ./model/final356-0.2435.hdf5\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.24350 to 0.24318, saving model to ./model/final357-0.2432.hdf5\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.24318 to 0.24300, saving model to ./model/final358-0.2430.hdf5\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.24300 to 0.24292, saving model to ./model/final359-0.2429.hdf5\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.24292\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.24292\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.24292\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.24292 to 0.24278, saving model to ./model/final363-0.2428.hdf5\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.24278 to 0.24248, saving model to ./model/final364-0.2425.hdf5\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.24248 to 0.24213, saving model to ./model/final365-0.2421.hdf5\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.24213 to 0.24181, saving model to ./model/final366-0.2418.hdf5\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.24181 to 0.24160, saving model to ./model/final367-0.2416.hdf5\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.24160 to 0.24149, saving model to ./model/final368-0.2415.hdf5\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.24149 to 0.24145, saving model to ./model/final369-0.2415.hdf5\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.24145 to 0.24141, saving model to ./model/final370-0.2414.hdf5\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.24141 to 0.24133, saving model to ./model/final371-0.2413.hdf5\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.24133 to 0.24116, saving model to ./model/final372-0.2412.hdf5\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.24116 to 0.24092, saving model to ./model/final373-0.2409.hdf5\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.24092 to 0.24066, saving model to ./model/final374-0.2407.hdf5\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.24066 to 0.24044, saving model to ./model/final375-0.2404.hdf5\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.24044 to 0.24027, saving model to ./model/final376-0.2403.hdf5\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.24027 to 0.24017, saving model to ./model/final377-0.2402.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00378: val_loss improved from 0.24017 to 0.24010, saving model to ./model/final378-0.2401.hdf5\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.24010 to 0.24002, saving model to ./model/final379-0.2400.hdf5\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.24002 to 0.23987, saving model to ./model/final380-0.2399.hdf5\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.23987 to 0.23965, saving model to ./model/final381-0.2397.hdf5\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.23965 to 0.23940, saving model to ./model/final382-0.2394.hdf5\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.23940 to 0.23917, saving model to ./model/final383-0.2392.hdf5\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.23917 to 0.23897, saving model to ./model/final384-0.2390.hdf5\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.23897 to 0.23879, saving model to ./model/final385-0.2388.hdf5\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.23879 to 0.23863, saving model to ./model/final386-0.2386.hdf5\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.23863 to 0.23843, saving model to ./model/final387-0.2384.hdf5\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.23843 to 0.23818, saving model to ./model/final388-0.2382.hdf5\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.23818 to 0.23793, saving model to ./model/final389-0.2379.hdf5\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.23793 to 0.23768, saving model to ./model/final390-0.2377.hdf5\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.23768 to 0.23746, saving model to ./model/final391-0.2375.hdf5\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.23746 to 0.23724, saving model to ./model/final392-0.2372.hdf5\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.23724 to 0.23706, saving model to ./model/final393-0.2371.hdf5\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.23706 to 0.23694, saving model to ./model/final394-0.2369.hdf5\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.23694 to 0.23686, saving model to ./model/final395-0.2369.hdf5\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.23686 to 0.23680, saving model to ./model/final396-0.2368.hdf5\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.23680 to 0.23672, saving model to ./model/final397-0.2367.hdf5\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.23672 to 0.23661, saving model to ./model/final398-0.2366.hdf5\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.23661 to 0.23645, saving model to ./model/final399-0.2365.hdf5\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.23645 to 0.23626, saving model to ./model/final400-0.2363.hdf5\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.23626 to 0.23605, saving model to ./model/final401-0.2360.hdf5\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.23605 to 0.23586, saving model to ./model/final402-0.2359.hdf5\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.23586 to 0.23564, saving model to ./model/final403-0.2356.hdf5\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.23564 to 0.23543, saving model to ./model/final404-0.2354.hdf5\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.23543 to 0.23525, saving model to ./model/final405-0.2352.hdf5\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.23525\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.23525\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.23525\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.23525 to 0.23478, saving model to ./model/final409-0.2348.hdf5\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.23478 to 0.23476, saving model to ./model/final410-0.2348.hdf5\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.23476\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.23476\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.23476\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.23476\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.23476 to 0.23456, saving model to ./model/final415-0.2346.hdf5\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.23456 to 0.23401, saving model to ./model/final416-0.2340.hdf5\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.23401 to 0.23363, saving model to ./model/final417-0.2336.hdf5\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.23363 to 0.23347, saving model to ./model/final418-0.2335.hdf5\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.23347\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.23347\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.23347\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.23347\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.23347\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.23347 to 0.23322, saving model to ./model/final424-0.2332.hdf5\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.23322 to 0.23237, saving model to ./model/final425-0.2324.hdf5\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.23237 to 0.23111, saving model to ./model/final426-0.2311.hdf5\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.23111 to 0.23011, saving model to ./model/final427-0.2301.hdf5\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.23011\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.23011 to 0.23005, saving model to ./model/final438-0.2301.hdf5\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.23005 to 0.22971, saving model to ./model/final439-0.2297.hdf5\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.22971\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.22971\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.22971\n",
      "\n",
      "Epoch 00443: val_loss improved from 0.22971 to 0.22957, saving model to ./model/final443-0.2296.hdf5\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.22957 to 0.22902, saving model to ./model/final444-0.2290.hdf5\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.22902 to 0.22857, saving model to ./model/final445-0.2286.hdf5\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.22857 to 0.22831, saving model to ./model/final446-0.2283.hdf5\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.22831 to 0.22827, saving model to ./model/final447-0.2283.hdf5\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.22827\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.22827\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.22827 to 0.22822, saving model to ./model/final450-0.2282.hdf5\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.22822 to 0.22795, saving model to ./model/final451-0.2280.hdf5\n",
      "\n",
      "Epoch 00452: val_loss improved from 0.22795 to 0.22765, saving model to ./model/final452-0.2276.hdf5\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.22765 to 0.22737, saving model to ./model/final453-0.2274.hdf5\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.22737 to 0.22716, saving model to ./model/final454-0.2272.hdf5\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.22716 to 0.22699, saving model to ./model/final455-0.2270.hdf5\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.22699 to 0.22682, saving model to ./model/final456-0.2268.hdf5\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.22682 to 0.22666, saving model to ./model/final457-0.2267.hdf5\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.22666 to 0.22654, saving model to ./model/final458-0.2265.hdf5\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.22654 to 0.22614, saving model to ./model/final459-0.2261.hdf5\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.22614\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.22614\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.22614\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.22614\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.22614 to 0.22570, saving model to ./model/final464-0.2257.hdf5\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.22570 to 0.22534, saving model to ./model/final465-0.2253.hdf5\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.22534\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.22534\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.22534\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.22534 to 0.22527, saving model to ./model/final469-0.2253.hdf5\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.22527 to 0.22470, saving model to ./model/final470-0.2247.hdf5\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.22470 to 0.22431, saving model to ./model/final471-0.2243.hdf5\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.22431\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.22431\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.22431\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.22431\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.22431 to 0.22406, saving model to ./model/final476-0.2241.hdf5\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.22406 to 0.22382, saving model to ./model/final477-0.2238.hdf5\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.22382\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.22382\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.22382\n",
      "\n",
      "Epoch 00481: val_loss improved from 0.22382 to 0.22336, saving model to ./model/final481-0.2234.hdf5\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.22336 to 0.22307, saving model to ./model/final482-0.2231.hdf5\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.22307\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.22307\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.22307\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.22307\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.22307 to 0.22277, saving model to ./model/final487-0.2228.hdf5\n",
      "\n",
      "Epoch 00488: val_loss improved from 0.22277 to 0.22199, saving model to ./model/final488-0.2220.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00489: val_loss improved from 0.22199 to 0.22190, saving model to ./model/final489-0.2219.hdf5\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.22190\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.22190\n",
      "\n",
      "Epoch 00492: val_loss improved from 0.22190 to 0.22189, saving model to ./model/final492-0.2219.hdf5\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.22189 to 0.22062, saving model to ./model/final493-0.2206.hdf5\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.22062 to 0.22003, saving model to ./model/final494-0.2200.hdf5\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.22003\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.22003 to 0.21975, saving model to ./model/final503-0.2198.hdf5\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.21975\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.21975\n",
      "\n",
      "Epoch 00506: val_loss improved from 0.21975 to 0.21956, saving model to ./model/final506-0.2196.hdf5\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.21956 to 0.21905, saving model to ./model/final507-0.2190.hdf5\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.21905 to 0.21865, saving model to ./model/final508-0.2187.hdf5\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.21865\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.21865\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.21865\n",
      "\n",
      "Epoch 00512: val_loss improved from 0.21865 to 0.21803, saving model to ./model/final512-0.2180.hdf5\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.21803\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.21803 to 0.21786, saving model to ./model/final519-0.2179.hdf5\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.21786\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.21786\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.21786\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.21786 to 0.21639, saving model to ./model/final523-0.2164.hdf5\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.21639 to 0.21576, saving model to ./model/final524-0.2158.hdf5\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.21576\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.21576\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.21576\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.21576 to 0.21525, saving model to ./model/final528-0.2152.hdf5\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.21525 to 0.21517, saving model to ./model/final529-0.2152.hdf5\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.21517\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.21517 to 0.21496, saving model to ./model/final537-0.2150.hdf5\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.21496\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.21496\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.21496 to 0.21465, saving model to ./model/final540-0.2147.hdf5\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.21465 to 0.21398, saving model to ./model/final541-0.2140.hdf5\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.21398\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.21398\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.21398\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.21398\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.21398 to 0.21327, saving model to ./model/final546-0.2133.hdf5\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.21327\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.21327\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.21327\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.21327\n",
      "\n",
      "Epoch 00551: val_loss improved from 0.21327 to 0.21308, saving model to ./model/final551-0.2131.hdf5\n",
      "\n",
      "Epoch 00552: val_loss improved from 0.21308 to 0.21250, saving model to ./model/final552-0.2125.hdf5\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.21250 to 0.21249, saving model to ./model/final553-0.2125.hdf5\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.21249\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.21249\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.21249\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.21249\n",
      "\n",
      "Epoch 00558: val_loss improved from 0.21249 to 0.21216, saving model to ./model/final558-0.2122.hdf5\n",
      "\n",
      "Epoch 00559: val_loss improved from 0.21216 to 0.21189, saving model to ./model/final559-0.2119.hdf5\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.21189\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.21189\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.21189\n",
      "\n",
      "Epoch 00563: val_loss improved from 0.21189 to 0.21166, saving model to ./model/final563-0.2117.hdf5\n",
      "\n",
      "Epoch 00564: val_loss improved from 0.21166 to 0.21115, saving model to ./model/final564-0.2112.hdf5\n",
      "\n",
      "Epoch 00565: val_loss improved from 0.21115 to 0.21089, saving model to ./model/final565-0.2109.hdf5\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.21089\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.21089\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.21089\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.21089\n",
      "\n",
      "Epoch 00570: val_loss improved from 0.21089 to 0.21054, saving model to ./model/final570-0.2105.hdf5\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.21054 to 0.21014, saving model to ./model/final571-0.2101.hdf5\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.21014\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.21014\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.21014\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.21014 to 0.20947, saving model to ./model/final575-0.2095.hdf5\n",
      "\n",
      "Epoch 00576: val_loss improved from 0.20947 to 0.20916, saving model to ./model/final576-0.2092.hdf5\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.20916\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.20916\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.20916\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.20916\n",
      "\n",
      "Epoch 00581: val_loss improved from 0.20916 to 0.20871, saving model to ./model/final581-0.2087.hdf5\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.20871\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.20871\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.20871\n",
      "\n",
      "Epoch 00585: val_loss improved from 0.20871 to 0.20798, saving model to ./model/final585-0.2080.hdf5\n",
      "\n",
      "Epoch 00586: val_loss improved from 0.20798 to 0.20765, saving model to ./model/final586-0.2076.hdf5\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00592: val_loss improved from 0.20765 to 0.20661, saving model to ./model/final592-0.2066.hdf5\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.20661 to 0.20528, saving model to ./model/final593-0.2053.hdf5\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.20528\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.20528\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.20528\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.20528\n",
      "\n",
      "Epoch 00598: val_loss improved from 0.20528 to 0.20318, saving model to ./model/final598-0.2032.hdf5\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.20318 to 0.20216, saving model to ./model/final599-0.2022.hdf5\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.20216\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.20216\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.20216\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.20216\n",
      "\n",
      "Epoch 00604: val_loss improved from 0.20216 to 0.20091, saving model to ./model/final604-0.2009.hdf5\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.20091\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.20091\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.20091\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.20091\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.20091\n",
      "\n",
      "Epoch 00610: val_loss improved from 0.20091 to 0.20014, saving model to ./model/final610-0.2001.hdf5\n",
      "\n",
      "Epoch 00611: val_loss improved from 0.20014 to 0.20012, saving model to ./model/final611-0.2001.hdf5\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.20012\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.20012\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.20012\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.20012\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.20012\n",
      "\n",
      "Epoch 00617: val_loss improved from 0.20012 to 0.19931, saving model to ./model/final617-0.1993.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00618: val_loss improved from 0.19931 to 0.19900, saving model to ./model/final618-0.1990.hdf5\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.19900\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.19900 to 0.19890, saving model to ./model/final620-0.1989.hdf5\n",
      "\n",
      "Epoch 00621: val_loss improved from 0.19890 to 0.19702, saving model to ./model/final621-0.1970.hdf5\n",
      "\n",
      "Epoch 00622: val_loss improved from 0.19702 to 0.19549, saving model to ./model/final622-0.1955.hdf5\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00627: val_loss improved from 0.19549 to 0.19524, saving model to ./model/final627-0.1952.hdf5\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.19524\n",
      "\n",
      "Epoch 00640: val_loss improved from 0.19524 to 0.19460, saving model to ./model/final640-0.1946.hdf5\n",
      "\n",
      "Epoch 00641: val_loss improved from 0.19460 to 0.19405, saving model to ./model/final641-0.1941.hdf5\n",
      "\n",
      "Epoch 00642: val_loss improved from 0.19405 to 0.19380, saving model to ./model/final642-0.1938.hdf5\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.19380\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.19380\n",
      "\n",
      "Epoch 00645: val_loss improved from 0.19380 to 0.19380, saving model to ./model/final645-0.1938.hdf5\n",
      "\n",
      "Epoch 00646: val_loss improved from 0.19380 to 0.19348, saving model to ./model/final646-0.1935.hdf5\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.19348 to 0.19337, saving model to ./model/final647-0.1934.hdf5\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.19337\n",
      "\n",
      "Epoch 00656: val_loss improved from 0.19337 to 0.19270, saving model to ./model/final656-0.1927.hdf5\n",
      "\n",
      "Epoch 00657: val_loss improved from 0.19270 to 0.19235, saving model to ./model/final657-0.1923.hdf5\n",
      "\n",
      "Epoch 00658: val_loss improved from 0.19235 to 0.19197, saving model to ./model/final658-0.1920.hdf5\n",
      "\n",
      "Epoch 00659: val_loss improved from 0.19197 to 0.19161, saving model to ./model/final659-0.1916.hdf5\n",
      "\n",
      "Epoch 00660: val_loss improved from 0.19161 to 0.19129, saving model to ./model/final660-0.1913.hdf5\n",
      "\n",
      "Epoch 00661: val_loss improved from 0.19129 to 0.19103, saving model to ./model/final661-0.1910.hdf5\n",
      "\n",
      "Epoch 00662: val_loss improved from 0.19103 to 0.19068, saving model to ./model/final662-0.1907.hdf5\n",
      "\n",
      "Epoch 00663: val_loss improved from 0.19068 to 0.19035, saving model to ./model/final663-0.1904.hdf5\n",
      "\n",
      "Epoch 00664: val_loss improved from 0.19035 to 0.19000, saving model to ./model/final664-0.1900.hdf5\n",
      "\n",
      "Epoch 00665: val_loss improved from 0.19000 to 0.18988, saving model to ./model/final665-0.1899.hdf5\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.18988\n",
      "\n",
      "Epoch 00667: val_loss improved from 0.18988 to 0.18972, saving model to ./model/final667-0.1897.hdf5\n",
      "\n",
      "Epoch 00668: val_loss improved from 0.18972 to 0.18949, saving model to ./model/final668-0.1895.hdf5\n",
      "\n",
      "Epoch 00669: val_loss improved from 0.18949 to 0.18946, saving model to ./model/final669-0.1895.hdf5\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.18946\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.18946\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.18946\n",
      "\n",
      "Epoch 00673: val_loss improved from 0.18946 to 0.18875, saving model to ./model/final673-0.1887.hdf5\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.18875\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.18875\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.18875\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.18875\n",
      "\n",
      "Epoch 00678: val_loss improved from 0.18875 to 0.18803, saving model to ./model/final678-0.1880.hdf5\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.18803 to 0.18720, saving model to ./model/final679-0.1872.hdf5\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.18720\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.18720\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.18720\n",
      "\n",
      "Epoch 00683: val_loss improved from 0.18720 to 0.18623, saving model to ./model/final683-0.1862.hdf5\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.18623\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.18623\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.18623\n",
      "\n",
      "Epoch 00687: val_loss improved from 0.18623 to 0.18447, saving model to ./model/final687-0.1845.hdf5\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.18447\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.18447\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.18447\n",
      "\n",
      "Epoch 00691: val_loss improved from 0.18447 to 0.18407, saving model to ./model/final691-0.1841.hdf5\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.18407\n",
      "\n",
      "Epoch 00704: val_loss improved from 0.18407 to 0.18307, saving model to ./model/final704-0.1831.hdf5\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.18307\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.18307\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.18307\n",
      "\n",
      "Epoch 00708: val_loss improved from 0.18307 to 0.18229, saving model to ./model/final708-0.1823.hdf5\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.18229\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.18229\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.18229\n",
      "\n",
      "Epoch 00712: val_loss improved from 0.18229 to 0.18152, saving model to ./model/final712-0.1815.hdf5\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.18152\n",
      "\n",
      "Epoch 00747: val_loss improved from 0.18152 to 0.18118, saving model to ./model/final747-0.1812.hdf5\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.18118\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.18118\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.18118\n",
      "\n",
      "Epoch 00751: val_loss improved from 0.18118 to 0.18046, saving model to ./model/final751-0.1805.hdf5\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.18046\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.18046\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.18046\n",
      "\n",
      "Epoch 00755: val_loss improved from 0.18046 to 0.18038, saving model to ./model/final755-0.1804.hdf5\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.18038\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.18038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00758: val_loss improved from 0.18038 to 0.17983, saving model to ./model/final758-0.1798.hdf5\n",
      "\n",
      "Epoch 00759: val_loss improved from 0.17983 to 0.17891, saving model to ./model/final759-0.1789.hdf5\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.17891\n",
      "\n",
      "Epoch 00766: val_loss improved from 0.17891 to 0.17875, saving model to ./model/final766-0.1787.hdf5\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.17875\n",
      "\n",
      "Epoch 00793: val_loss improved from 0.17875 to 0.17864, saving model to ./model/final793-0.1786.hdf5\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.17864\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.17864\n",
      "\n",
      "Epoch 00796: val_loss improved from 0.17864 to 0.17854, saving model to ./model/final796-0.1785.hdf5\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.17854\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.17854\n",
      "\n",
      "Epoch 00799: val_loss improved from 0.17854 to 0.17849, saving model to ./model/final799-0.1785.hdf5\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.17849\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.17849\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.17849\n",
      "\n",
      "Epoch 00803: val_loss improved from 0.17849 to 0.17832, saving model to ./model/final803-0.1783.hdf5\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.17832\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.17832\n",
      "\n",
      "Epoch 00806: val_loss improved from 0.17832 to 0.17800, saving model to ./model/final806-0.1780.hdf5\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.17800\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.17800\n",
      "\n",
      "Epoch 00809: val_loss improved from 0.17800 to 0.17753, saving model to ./model/final809-0.1775.hdf5\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.17753\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.17753\n",
      "\n",
      "Epoch 00812: val_loss improved from 0.17753 to 0.17710, saving model to ./model/final812-0.1771.hdf5\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.17710\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.17710\n",
      "\n",
      "Epoch 00815: val_loss improved from 0.17710 to 0.17683, saving model to ./model/final815-0.1768.hdf5\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.17683\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.17683\n",
      "\n",
      "Epoch 00818: val_loss improved from 0.17683 to 0.17673, saving model to ./model/final818-0.1767.hdf5\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.17673\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.17673\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.17673\n",
      "\n",
      "Epoch 00822: val_loss improved from 0.17673 to 0.17641, saving model to ./model/final822-0.1764.hdf5\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.17641\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.17641\n",
      "\n",
      "Epoch 00825: val_loss improved from 0.17641 to 0.17614, saving model to ./model/final825-0.1761.hdf5\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.17614\n",
      "\n",
      "Epoch 00839: val_loss improved from 0.17614 to 0.17455, saving model to ./model/final839-0.1745.hdf5\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.17455\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.17455\n",
      "\n",
      "Epoch 00842: val_loss improved from 0.17455 to 0.17345, saving model to ./model/final842-0.1735.hdf5\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.17345\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.17345\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.17345\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.17345\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.17345\n",
      "\n",
      "Epoch 00848: val_loss improved from 0.17345 to 0.17282, saving model to ./model/final848-0.1728.hdf5\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.17282\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.17282\n",
      "\n",
      "Epoch 00851: val_loss improved from 0.17282 to 0.17198, saving model to ./model/final851-0.1720.hdf5\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.17198\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.17198\n",
      "\n",
      "Epoch 00854: val_loss improved from 0.17198 to 0.17121, saving model to ./model/final854-0.1712.hdf5\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.17121\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.17121\n",
      "\n",
      "Epoch 00857: val_loss improved from 0.17121 to 0.17052, saving model to ./model/final857-0.1705.hdf5\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.17052\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.17052\n",
      "\n",
      "Epoch 00860: val_loss improved from 0.17052 to 0.16986, saving model to ./model/final860-0.1699.hdf5\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.16986\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.16986\n",
      "\n",
      "Epoch 00863: val_loss improved from 0.16986 to 0.16926, saving model to ./model/final863-0.1693.hdf5\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.16926\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.16926\n",
      "\n",
      "Epoch 00866: val_loss improved from 0.16926 to 0.16875, saving model to ./model/final866-0.1687.hdf5\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.16875\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.16875\n",
      "\n",
      "Epoch 00869: val_loss improved from 0.16875 to 0.16836, saving model to ./model/final869-0.1684.hdf5\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.16836\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.16836\n",
      "\n",
      "Epoch 00872: val_loss improved from 0.16836 to 0.16603, saving model to ./model/final872-0.1660.hdf5\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.16603\n",
      "\n",
      "Epoch 00883: val_loss improved from 0.16603 to 0.16424, saving model to ./model/final883-0.1642.hdf5\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.16424\n",
      "\n",
      "Epoch 00894: val_loss improved from 0.16424 to 0.16381, saving model to ./model/final894-0.1638.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00895: val_loss did not improve from 0.16381\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.16381\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.16381\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.16381\n",
      "\n",
      "Epoch 00899: val_loss improved from 0.16381 to 0.16255, saving model to ./model/final899-0.1626.hdf5\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.16255\n",
      "\n",
      "Epoch 00910: val_loss improved from 0.16255 to 0.16088, saving model to ./model/final910-0.1609.hdf5\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.16088\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.16088\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.16088\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.16088\n",
      "\n",
      "Epoch 00915: val_loss improved from 0.16088 to 0.16065, saving model to ./model/final915-0.1607.hdf5\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.16065\n",
      "\n",
      "Epoch 00931: val_loss improved from 0.16065 to 0.15890, saving model to ./model/final931-0.1589.hdf5\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.15890\n",
      "\n",
      "Epoch 00947: val_loss improved from 0.15890 to 0.15717, saving model to ./model/final947-0.1572.hdf5\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.15717\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.15717\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.15717\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.15717\n",
      "\n",
      "Epoch 00952: val_loss improved from 0.15717 to 0.15663, saving model to ./model/final952-0.1566.hdf5\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.15663\n",
      "\n",
      "Epoch 00954: val_loss improved from 0.15663 to 0.15637, saving model to ./model/final954-0.1564.hdf5\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.15637\n",
      "\n",
      "Epoch 00961: val_loss improved from 0.15637 to 0.15602, saving model to ./model/final961-0.1560.hdf5\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.15602\n",
      "\n",
      "Epoch 00968: val_loss improved from 0.15602 to 0.15351, saving model to ./model/final968-0.1535.hdf5\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.15351\n",
      "\n",
      "Epoch 00975: val_loss improved from 0.15351 to 0.15216, saving model to ./model/final975-0.1522.hdf5\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.15216\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.15216\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.15216\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.15216\n",
      "\n",
      "Epoch 00980: val_loss improved from 0.15216 to 0.15102, saving model to ./model/final980-0.1510.hdf5\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.15102\n",
      "\n",
      "Epoch 00982: val_loss improved from 0.15102 to 0.15084, saving model to ./model/final982-0.1508.hdf5\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.15084\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.15084\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.15084\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.15084\n",
      "\n",
      "Epoch 00987: val_loss improved from 0.15084 to 0.14975, saving model to ./model/final987-0.1498.hdf5\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.14975\n",
      "\n",
      "Epoch 01005: val_loss improved from 0.14975 to 0.14971, saving model to ./model/final1005-0.1497.hdf5\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.14971\n",
      "\n",
      "Epoch 01025: val_loss improved from 0.14971 to 0.14894, saving model to ./model/final1025-0.1489.hdf5\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.14894\n",
      "\n",
      "Epoch 01027: val_loss improved from 0.14894 to 0.14759, saving model to ./model/final1027-0.1476.hdf5\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.14759\n",
      "\n",
      "Epoch 01037: val_loss improved from 0.14759 to 0.14699, saving model to ./model/final1037-0.1470.hdf5\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.14699\n",
      "\n",
      "Epoch 01039: val_loss improved from 0.14699 to 0.14623, saving model to ./model/final1039-0.1462.hdf5\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.14623\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.14623\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.14623\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.14623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01044: val_loss did not improve from 0.14623\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.14623\n",
      "\n",
      "Epoch 01046: val_loss improved from 0.14623 to 0.14592, saving model to ./model/final1046-0.1459.hdf5\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.14592\n",
      "\n",
      "Epoch 01048: val_loss improved from 0.14592 to 0.14545, saving model to ./model/final1048-0.1454.hdf5\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.14545\n",
      "\n",
      "Epoch 01058: val_loss improved from 0.14545 to 0.14508, saving model to ./model/final1058-0.1451.hdf5\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.14508\n",
      "\n",
      "Epoch 01060: val_loss improved from 0.14508 to 0.14391, saving model to ./model/final1060-0.1439.hdf5\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.14391\n",
      "\n",
      "Epoch 01062: val_loss improved from 0.14391 to 0.14362, saving model to ./model/final1062-0.1436.hdf5\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.14362\n",
      "\n",
      "Epoch 01084: val_loss improved from 0.14362 to 0.13973, saving model to ./model/final1084-0.1397.hdf5\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.13973\n",
      "\n",
      "Epoch 01086: val_loss improved from 0.13973 to 0.13720, saving model to ./model/final1086-0.1372.hdf5\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.13720\n",
      "\n",
      "Epoch 01088: val_loss improved from 0.13720 to 0.13663, saving model to ./model/final1088-0.1366.hdf5\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.13663\n",
      "\n",
      "Epoch 01108: val_loss improved from 0.13663 to 0.13479, saving model to ./model/final1108-0.1348.hdf5\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.13479\n",
      "\n",
      "Epoch 01110: val_loss improved from 0.13479 to 0.13233, saving model to ./model/final1110-0.1323.hdf5\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.13233\n",
      "\n",
      "Epoch 01112: val_loss improved from 0.13233 to 0.13205, saving model to ./model/final1112-0.1320.hdf5\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 01150: val_loss improved from 0.13205 to 0.13152, saving model to ./model/final1150-0.1315.hdf5\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.13152\n",
      "\n",
      "Epoch 01152: val_loss improved from 0.13152 to 0.12936, saving model to ./model/final1152-0.1294.hdf5\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.12936\n",
      "\n",
      "Epoch 01171: val_loss improved from 0.12936 to 0.12796, saving model to ./model/final1171-0.1280.hdf5\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.12796\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01200: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.12796\n",
      "\n",
      "Epoch 01219: val_loss improved from 0.12796 to 0.12500, saving model to ./model/final1219-0.1250.hdf5\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 0.12500\n",
      "\n",
      "Epoch 01221: val_loss improved from 0.12500 to 0.12308, saving model to ./model/final1221-0.1231.hdf5\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.12308\n",
      "\n",
      "Epoch 01242: val_loss improved from 0.12308 to 0.12211, saving model to ./model/final1242-0.1221.hdf5\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.12211\n",
      "\n",
      "Epoch 01309: val_loss improved from 0.12211 to 0.11897, saving model to ./model/final1309-0.1190.hdf5\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.11897\n",
      "\n",
      "Epoch 01311: val_loss improved from 0.11897 to 0.11618, saving model to ./model/final1311-0.1162.hdf5\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.11618\n",
      "\n",
      "Epoch 01353: val_loss improved from 0.11618 to 0.11531, saving model to ./model/final1353-0.1153.hdf5\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.11531\n",
      "\n",
      "Epoch 01355: val_loss improved from 0.11531 to 0.11506, saving model to ./model/final1355-0.1151.hdf5\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.11506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01357: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.11506\n",
      "\n",
      "Epoch 01427: val_loss improved from 0.11506 to 0.11410, saving model to ./model/final1427-0.1141.hdf5\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 0.11410\n",
      "\n",
      "Epoch 01429: val_loss improved from 0.11410 to 0.11197, saving model to ./model/final1429-0.1120.hdf5\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.11197\n",
      "\n",
      "Epoch 01431: val_loss improved from 0.11197 to 0.11185, saving model to ./model/final1431-0.1119.hdf5\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.11185\n",
      "\n",
      "Epoch 01475: val_loss improved from 0.11185 to 0.11169, saving model to ./model/final1475-0.1117.hdf5\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 0.11169\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01532: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 0.11169\n",
      "\n",
      "Epoch 01558: val_loss improved from 0.11169 to 0.10536, saving model to ./model/final1558-0.1054.hdf5\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 0.10536\n",
      "\n",
      "Epoch 01560: val_loss improved from 0.10536 to 0.10034, saving model to ./model/final1560-0.1003.hdf5\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 0.10034\n",
      "\n",
      "Epoch 01562: val_loss improved from 0.10034 to 0.09609, saving model to ./model/final1562-0.0961.hdf5\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01593: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01630: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01638: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01652: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01654: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 0.09609\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 0.09609\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=3500, batch_size=500,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 - 0s - loss: 0.0612 - accuracy: 0.9825\n",
      "\n",
      " Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = load_model('model/final1562-0.0961.hdf5')\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, y_test, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정\n",
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "Y_obj = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3071437902148591, 0.3766279093674481)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "np.mean(X_scaled), np.std(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_obj[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "Y = np_utils.to_categorical(Y_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터셋 테스트데이터셋 (8:2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 600)               39000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                6010      \n",
      "=================================================================\n",
      "Total params: 766,210\n",
      "Trainable params: 766,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "model = Sequential([\n",
    "    Dense(600, input_dim=64, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(10, activation='sigmoid')\n",
    "]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "import os\n",
    "MODEL_DIR = './model/digits/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 조건 설정\n",
    "modelpath = MODEL_DIR + \"final{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "checkpointer_callback = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.04914, saving model to ./model/digits/final001-2.0491.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.04914 to 1.58582, saving model to ./model/digits/final002-1.5858.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58582 to 0.83395, saving model to ./model/digits/final003-0.8339.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.83395 to 0.46896, saving model to ./model/digits/final004-0.4690.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46896 to 0.39118, saving model to ./model/digits/final005-0.3912.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.39118 to 0.21853, saving model to ./model/digits/final006-0.2185.hdf5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21853\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21853 to 0.18017, saving model to ./model/digits/final008-0.1802.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.18017 to 0.14463, saving model to ./model/digits/final009-0.1446.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14463 to 0.14154, saving model to ./model/digits/final010-0.1415.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14154\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14154\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14154 to 0.11718, saving model to ./model/digits/final013-0.1172.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11718 to 0.10472, saving model to ./model/digits/final014-0.1047.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10472\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.10472 to 0.10356, saving model to ./model/digits/final016-0.1036.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10356 to 0.09366, saving model to ./model/digits/final017-0.0937.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09366\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09366\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09366\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09366 to 0.09365, saving model to ./model/digits/final021-0.0936.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09365 to 0.08381, saving model to ./model/digits/final022-0.0838.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08381 to 0.08092, saving model to ./model/digits/final023-0.0809.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08092\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08092\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.08092 to 0.07906, saving model to ./model/digits/final026-0.0791.hdf5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07906\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07906\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07906 to 0.07622, saving model to ./model/digits/final029-0.0762.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.07622\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07622\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.07622\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.07622 to 0.07588, saving model to ./model/digits/final033-0.0759.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07588 to 0.07282, saving model to ./model/digits/final034-0.0728.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07282\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행 및 저장\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=5000, batch_size=400,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 - 0s - loss: 0.0689 - accuracy: 0.9833\n",
      "\n",
      " Accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = load_model('model/digits/final034-0.0728.hdf5')\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, y_test, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(n_hidden=3, n_neurons=100, learning_rate=3e-3,input_shape=[64]):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "keras_clf = KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 562us/sample - loss: 1.2982 - accuracy: 0.5457 - val_loss: 0.6133 - val_accuracy: 0.7552\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 111us/sample - loss: 0.4688 - accuracy: 0.8616 - val_loss: 0.4174 - val_accuracy: 0.8698\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.2112 - accuracy: 0.9360 - val_loss: 0.2366 - val_accuracy: 0.9062\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.2110 - accuracy: 0.9321 - val_loss: 0.3287 - val_accuracy: 0.8958\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.2244 - accuracy: 0.9334 - val_loss: 0.2463 - val_accuracy: 0.9115\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.1269 - accuracy: 0.9634 - val_loss: 0.1886 - val_accuracy: 0.9219\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0896 - accuracy: 0.9726 - val_loss: 0.1021 - val_accuracy: 0.9740\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.0845 - accuracy: 0.9713 - val_loss: 0.4167 - val_accuracy: 0.9115\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.2132 - accuracy: 0.9439 - val_loss: 0.2034 - val_accuracy: 0.9583\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.1114 - accuracy: 0.9713 - val_loss: 0.2425 - val_accuracy: 0.9635\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0972 - accuracy: 0.9752 - val_loss: 0.5886 - val_accuracy: 0.9062\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.1361 - accuracy: 0.9713 - val_loss: 0.1536 - val_accuracy: 0.9635\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.0756 - accuracy: 0.9700 - val_loss: 0.0895 - val_accuracy: 0.9740\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.0911 - accuracy: 0.9752 - val_loss: 0.6968 - val_accuracy: 0.8854\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.2544 - accuracy: 0.9347 - val_loss: 0.5709 - val_accuracy: 0.9062\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 96us/sample - loss: 0.3293 - accuracy: 0.9269 - val_loss: 0.2099 - val_accuracy: 0.8698\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.1321 - accuracy: 0.8734 - val_loss: 0.2103 - val_accuracy: 0.8542\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.2152 - accuracy: 0.8708 - val_loss: 0.7060 - val_accuracy: 0.7188\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.5241 - accuracy: 0.8081 - val_loss: 0.2774 - val_accuracy: 0.8177\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.2401 - accuracy: 0.8577 - val_loss: 0.5853 - val_accuracy: 0.8646\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.1465 - accuracy: 0.8903 - val_loss: 0.4022 - val_accuracy: 0.8438\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 96us/sample - loss: 0.1218 - accuracy: 0.8930 - val_loss: 0.3698 - val_accuracy: 0.8906\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.1099 - accuracy: 0.9060 - val_loss: 0.2548 - val_accuracy: 0.8698\n",
      "479/479 [==============================] - 0s 173us/sample - loss: 0.4342 - accuracy: 0.8956\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 573us/sample - loss: 1.6409 - accuracy: 0.4321 - val_loss: 0.6895 - val_accuracy: 0.7083\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.5970 - accuracy: 0.8211 - val_loss: 0.5903 - val_accuracy: 0.8594\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.3208 - accuracy: 0.8956 - val_loss: 0.3469 - val_accuracy: 0.9115\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 0.2053 - accuracy: 0.9347 - val_loss: 0.1858 - val_accuracy: 0.9271\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.1070 - accuracy: 0.9621 - val_loss: 0.3013 - val_accuracy: 0.9219\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.2043 - accuracy: 0.9491 - val_loss: 0.1693 - val_accuracy: 0.9531\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 106us/sample - loss: 0.0631 - accuracy: 0.9856 - val_loss: 0.4124 - val_accuracy: 0.8958\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 0.1419 - accuracy: 0.9504 - val_loss: 0.1502 - val_accuracy: 0.9583\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0991 - accuracy: 0.9687 - val_loss: 0.1227 - val_accuracy: 0.9531\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.1102 - accuracy: 0.9713 - val_loss: 0.1157 - val_accuracy: 0.9479\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.0753 - accuracy: 0.9804 - val_loss: 0.1880 - val_accuracy: 0.9427\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0267 - accuracy: 0.9896 - val_loss: 0.2809 - val_accuracy: 0.9479\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.1201 - accuracy: 0.9674 - val_loss: 0.1616 - val_accuracy: 0.9531\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.0644 - accuracy: 0.9804 - val_loss: 0.1885 - val_accuracy: 0.9323\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.0715 - accuracy: 0.9843 - val_loss: 0.0562 - val_accuracy: 0.9948\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.2238 - accuracy: 0.9530 - val_loss: 0.5408 - val_accuracy: 0.8750\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.2839 - accuracy: 0.9321 - val_loss: 0.6437 - val_accuracy: 0.8750\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.2535 - accuracy: 0.9465 - val_loss: 0.3947 - val_accuracy: 0.9427\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.2577 - accuracy: 0.9465 - val_loss: 0.5155 - val_accuracy: 0.8854\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.3549 - accuracy: 0.9373 - val_loss: 0.5274 - val_accuracy: 0.9115\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 92us/sample - loss: 0.3741 - accuracy: 0.8446 - val_loss: 0.4831 - val_accuracy: 0.8073\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.3129 - accuracy: 0.8695 - val_loss: 0.2603 - val_accuracy: 0.8333\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.1946 - accuracy: 0.8930 - val_loss: 0.2969 - val_accuracy: 0.8333\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.1476 - accuracy: 0.9308 - val_loss: 0.2323 - val_accuracy: 0.8958\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.0829 - accuracy: 0.9465 - val_loss: 0.2120 - val_accuracy: 0.9323\n",
      "479/479 [==============================] - 0s 417us/sample - loss: 0.2969 - accuracy: 0.9436\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 544us/sample - loss: 1.3443 - accuracy: 0.5170 - val_loss: 0.7754 - val_accuracy: 0.6719\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 108us/sample - loss: 0.6708 - accuracy: 0.8211 - val_loss: 0.5615 - val_accuracy: 0.8021\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.3967 - accuracy: 0.8890 - val_loss: 0.3421 - val_accuracy: 0.9271\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.2398 - accuracy: 0.9386 - val_loss: 0.2942 - val_accuracy: 0.9219\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.1517 - accuracy: 0.9556 - val_loss: 0.1918 - val_accuracy: 0.9531\n",
      "Epoch 6/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 97us/sample - loss: 0.1013 - accuracy: 0.9726 - val_loss: 0.2521 - val_accuracy: 0.9375\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.0841 - accuracy: 0.9765 - val_loss: 0.2922 - val_accuracy: 0.9271\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 0.1093 - accuracy: 0.9687 - val_loss: 0.2991 - val_accuracy: 0.9271\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.1556 - accuracy: 0.9661 - val_loss: 0.2778 - val_accuracy: 0.9479\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.1381 - accuracy: 0.9634 - val_loss: 0.6107 - val_accuracy: 0.9062\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 0.1969 - accuracy: 0.9582 - val_loss: 0.1678 - val_accuracy: 0.9479\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.1181 - accuracy: 0.9778 - val_loss: 0.3271 - val_accuracy: 0.9167\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 0.1535 - accuracy: 0.9582 - val_loss: 0.1429 - val_accuracy: 0.9427\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.1317 - accuracy: 0.9569 - val_loss: 0.3986 - val_accuracy: 0.9062\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.0595 - accuracy: 0.9804 - val_loss: 0.3571 - val_accuracy: 0.9375\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.0661 - accuracy: 0.9869 - val_loss: 0.6207 - val_accuracy: 0.8854\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.2389 - accuracy: 0.9478 - val_loss: 0.2882 - val_accuracy: 0.9479\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.1258 - accuracy: 0.9739 - val_loss: 0.5749 - val_accuracy: 0.9375\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.1388 - accuracy: 0.9608 - val_loss: 0.4059 - val_accuracy: 0.9323\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.2883 - accuracy: 0.9504 - val_loss: 0.3007 - val_accuracy: 0.9323\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.1081 - accuracy: 0.9648 - val_loss: 0.5601 - val_accuracy: 0.9115\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.1799 - accuracy: 0.9582 - val_loss: 0.3235 - val_accuracy: 0.9427\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.3707 - accuracy: 0.8930 - val_loss: 0.4297 - val_accuracy: 0.8698\n",
      "479/479 [==============================] - 0s 152us/sample - loss: 0.2809 - accuracy: 0.8747\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 913us/sample - loss: 1.8422 - accuracy: 0.5261 - val_loss: 0.7060 - val_accuracy: 0.8229\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 413us/sample - loss: 0.4845 - accuracy: 0.8551 - val_loss: 0.4665 - val_accuracy: 0.8438\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 402us/sample - loss: 0.2442 - accuracy: 0.9217 - val_loss: 0.2127 - val_accuracy: 0.9375\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 394us/sample - loss: 0.1462 - accuracy: 0.9465 - val_loss: 0.1495 - val_accuracy: 0.9479\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 399us/sample - loss: 0.0817 - accuracy: 0.9791 - val_loss: 0.1462 - val_accuracy: 0.9531\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 387us/sample - loss: 0.0526 - accuracy: 0.9856 - val_loss: 0.1284 - val_accuracy: 0.9531\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 382us/sample - loss: 0.0302 - accuracy: 0.9948 - val_loss: 0.1363 - val_accuracy: 0.9531\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 390us/sample - loss: 0.0175 - accuracy: 0.9974 - val_loss: 0.0674 - val_accuracy: 0.9740\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 382us/sample - loss: 0.0166 - accuracy: 0.9974 - val_loss: 0.0787 - val_accuracy: 0.9635\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 376us/sample - loss: 0.0221 - accuracy: 0.9935 - val_loss: 0.1318 - val_accuracy: 0.9479\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 392us/sample - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.0737 - val_accuracy: 0.9740\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 402us/sample - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0887 - val_accuracy: 0.9583\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 377us/sample - loss: 0.0096 - accuracy: 0.9987 - val_loss: 0.1118 - val_accuracy: 0.9531\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 374us/sample - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0590 - val_accuracy: 0.9792\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 392us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9740\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 391us/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9792\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 387us/sample - loss: 7.8833e-04 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9792\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 444us/sample - loss: 6.8730e-04 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9844\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 404us/sample - loss: 5.9154e-04 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9844\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 391us/sample - loss: 5.4546e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9844\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 395us/sample - loss: 4.7830e-04 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9844\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 401us/sample - loss: 4.4569e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9844\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 397us/sample - loss: 4.0562e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 413us/sample - loss: 3.6414e-04 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9844\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 388us/sample - loss: 3.3932e-04 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9844\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 386us/sample - loss: 3.1243e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9792\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 405us/sample - loss: 2.9121e-04 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9844\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 372us/sample - loss: 2.7437e-04 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9844\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 376us/sample - loss: 2.5401e-04 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9844\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 380us/sample - loss: 2.4095e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9844\n",
      "Epoch 31/2000\n",
      "766/766 [==============================] - 0s 354us/sample - loss: 2.2547e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
      "Epoch 32/2000\n",
      "766/766 [==============================] - 0s 389us/sample - loss: 2.1648e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844\n",
      "Epoch 33/2000\n",
      "766/766 [==============================] - 0s 361us/sample - loss: 2.0172e-04 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9844\n",
      "Epoch 34/2000\n",
      "766/766 [==============================] - 0s 356us/sample - loss: 1.9057e-04 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9844\n",
      "Epoch 35/2000\n",
      "766/766 [==============================] - 0s 388us/sample - loss: 1.7815e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844\n",
      "Epoch 36/2000\n",
      "766/766 [==============================] - 0s 398us/sample - loss: 1.7186e-04 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/2000\n",
      "766/766 [==============================] - 0s 378us/sample - loss: 1.6064e-04 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9844\n",
      "Epoch 38/2000\n",
      "766/766 [==============================] - 0s 356us/sample - loss: 1.5253e-04 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 0.9844\n",
      "Epoch 39/2000\n",
      "766/766 [==============================] - 0s 347us/sample - loss: 1.4418e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9844\n",
      "Epoch 40/2000\n",
      "766/766 [==============================] - 0s 390us/sample - loss: 1.3993e-04 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9844\n",
      "Epoch 41/2000\n",
      "766/766 [==============================] - 0s 362us/sample - loss: 1.3199e-04 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9844\n",
      "Epoch 42/2000\n",
      "766/766 [==============================] - 0s 402us/sample - loss: 1.2626e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
      "Epoch 43/2000\n",
      "766/766 [==============================] - 0s 362us/sample - loss: 1.1656e-04 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9844\n",
      "Epoch 44/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 1.1083e-04 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9844\n",
      "Epoch 45/2000\n",
      "766/766 [==============================] - 0s 376us/sample - loss: 1.0488e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9844\n",
      "Epoch 46/2000\n",
      "766/766 [==============================] - 0s 366us/sample - loss: 1.0096e-04 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9844\n",
      "Epoch 47/2000\n",
      "766/766 [==============================] - 0s 408us/sample - loss: 9.5798e-05 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9844\n",
      "Epoch 48/2000\n",
      "766/766 [==============================] - 0s 365us/sample - loss: 9.0802e-05 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9844\n",
      "Epoch 49/2000\n",
      "766/766 [==============================] - 0s 382us/sample - loss: 8.6698e-05 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9844\n",
      "Epoch 50/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 8.2231e-05 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9844\n",
      "Epoch 51/2000\n",
      "766/766 [==============================] - 0s 399us/sample - loss: 7.9050e-05 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9844\n",
      "Epoch 52/2000\n",
      "766/766 [==============================] - 0s 383us/sample - loss: 7.5252e-05 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9844\n",
      "Epoch 53/2000\n",
      "766/766 [==============================] - 0s 352us/sample - loss: 7.2337e-05 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
      "Epoch 54/2000\n",
      "766/766 [==============================] - 0s 377us/sample - loss: 6.9090e-05 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9844\n",
      "Epoch 55/2000\n",
      "766/766 [==============================] - 0s 377us/sample - loss: 6.6934e-05 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9844\n",
      "Epoch 56/2000\n",
      "766/766 [==============================] - 0s 372us/sample - loss: 6.3742e-05 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9844\n",
      "Epoch 57/2000\n",
      "766/766 [==============================] - 0s 349us/sample - loss: 6.1595e-05 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
      "Epoch 58/2000\n",
      "766/766 [==============================] - 0s 369us/sample - loss: 5.9131e-05 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9844\n",
      "Epoch 59/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 5.7160e-05 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9844\n",
      "Epoch 60/2000\n",
      "766/766 [==============================] - 0s 371us/sample - loss: 5.4796e-05 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9844\n",
      "Epoch 61/2000\n",
      "766/766 [==============================] - 0s 448us/sample - loss: 5.2022e-05 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9844\n",
      "Epoch 62/2000\n",
      "766/766 [==============================] - 0s 431us/sample - loss: 5.0421e-05 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9844\n",
      "Epoch 63/2000\n",
      "766/766 [==============================] - 0s 411us/sample - loss: 4.8818e-05 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9844\n",
      "479/479 [==============================] - 0s 229us/sample - loss: 0.1739 - accuracy: 0.9666\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 917us/sample - loss: 1.6830 - accuracy: 0.5692 - val_loss: 0.7459 - val_accuracy: 0.7969\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 392us/sample - loss: 0.4589 - accuracy: 0.8590 - val_loss: 0.6366 - val_accuracy: 0.7917\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 383us/sample - loss: 0.2880 - accuracy: 0.9086 - val_loss: 0.3207 - val_accuracy: 0.8802\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 352us/sample - loss: 0.1436 - accuracy: 0.9582 - val_loss: 0.1660 - val_accuracy: 0.9427\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 371us/sample - loss: 0.1103 - accuracy: 0.9700 - val_loss: 0.2163 - val_accuracy: 0.9167\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0587 - accuracy: 0.9817 - val_loss: 0.0829 - val_accuracy: 0.9635\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 371us/sample - loss: 0.0531 - accuracy: 0.9804 - val_loss: 0.1613 - val_accuracy: 0.9323\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0549 - accuracy: 0.9830 - val_loss: 0.0968 - val_accuracy: 0.9635\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 371us/sample - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.0794 - val_accuracy: 0.9844\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0325 - accuracy: 0.9922 - val_loss: 0.1197 - val_accuracy: 0.9531\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 370us/sample - loss: 0.0288 - accuracy: 0.9935 - val_loss: 0.0873 - val_accuracy: 0.9635\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 0.9583\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 372us/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9688\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9583\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 382us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9688\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 372us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 372us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 394us/sample - loss: 8.9646e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9583\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 355us/sample - loss: 7.9087e-04 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 378us/sample - loss: 7.1565e-04 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9583\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 6.5929e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 371us/sample - loss: 5.5547e-04 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 356us/sample - loss: 5.0071e-04 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9635\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 406us/sample - loss: 4.5139e-04 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9635\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 356us/sample - loss: 4.0844e-04 - accuracy: 1.0000 - val_loss: 0.0868 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 242us/sample - loss: 0.0949 - accuracy: 0.9729\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 1s 910us/sample - loss: 1.7651 - accuracy: 0.6018 - val_loss: 0.7482 - val_accuracy: 0.7969\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 393us/sample - loss: 0.4635 - accuracy: 0.8420 - val_loss: 0.3701 - val_accuracy: 0.8958\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 394us/sample - loss: 0.2512 - accuracy: 0.9230 - val_loss: 0.2992 - val_accuracy: 0.9010\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 497us/sample - loss: 0.1594 - accuracy: 0.9491 - val_loss: 0.2692 - val_accuracy: 0.9219\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 511us/sample - loss: 0.1212 - accuracy: 0.9595 - val_loss: 0.1723 - val_accuracy: 0.9375\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 368us/sample - loss: 0.0660 - accuracy: 0.9804 - val_loss: 0.1404 - val_accuracy: 0.9583\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 369us/sample - loss: 0.0548 - accuracy: 0.9804 - val_loss: 0.2114 - val_accuracy: 0.9375\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 361us/sample - loss: 0.0407 - accuracy: 0.9896 - val_loss: 0.1461 - val_accuracy: 0.9583\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 394us/sample - loss: 0.0145 - accuracy: 0.9987 - val_loss: 0.1192 - val_accuracy: 0.9688\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9635\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 370us/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9635\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 390us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9740\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9688\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 375us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 375us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9688\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 367us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 362us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1249 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 396us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 398us/sample - loss: 9.7863e-04 - accuracy: 1.0000 - val_loss: 0.1274 - val_accuracy: 0.9688\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 413us/sample - loss: 8.2509e-04 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 393us/sample - loss: 7.3412e-04 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 366us/sample - loss: 6.6580e-04 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 0.9688\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 375us/sample - loss: 5.9402e-04 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 271us/sample - loss: 0.1123 - accuracy: 0.9645\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 541us/sample - loss: 2.2248 - accuracy: 0.3042 - val_loss: 2.0715 - val_accuracy: 0.5938\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 45us/sample - loss: 1.5302 - accuracy: 0.7324 - val_loss: 0.8123 - val_accuracy: 0.7760\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.5242 - accuracy: 0.8446 - val_loss: 0.3403 - val_accuracy: 0.9062\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.2740 - accuracy: 0.9230 - val_loss: 0.2314 - val_accuracy: 0.9219\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.1799 - accuracy: 0.9504 - val_loss: 0.2232 - val_accuracy: 0.9271\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 59us/sample - loss: 0.1336 - accuracy: 0.9556 - val_loss: 0.1424 - val_accuracy: 0.9740\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 48us/sample - loss: 0.1209 - accuracy: 0.9582 - val_loss: 0.2125 - val_accuracy: 0.9167\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 49us/sample - loss: 0.0977 - accuracy: 0.9661 - val_loss: 0.1325 - val_accuracy: 0.9583\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 54us/sample - loss: 0.0659 - accuracy: 0.9883 - val_loss: 0.1153 - val_accuracy: 0.9479\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 56us/sample - loss: 0.0511 - accuracy: 0.9948 - val_loss: 0.1057 - val_accuracy: 0.9635\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 40us/sample - loss: 0.0364 - accuracy: 0.9974 - val_loss: 0.1134 - val_accuracy: 0.9531\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0283 - accuracy: 0.9974 - val_loss: 0.1127 - val_accuracy: 0.9583\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 49us/sample - loss: 0.0250 - accuracy: 0.9961 - val_loss: 0.1029 - val_accuracy: 0.9688\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 58us/sample - loss: 0.0281 - accuracy: 0.9961 - val_loss: 0.0669 - val_accuracy: 0.9896\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 47us/sample - loss: 0.0282 - accuracy: 0.9909 - val_loss: 0.1691 - val_accuracy: 0.9219\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9688\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 59us/sample - loss: 0.0128 - accuracy: 0.9987 - val_loss: 0.1034 - val_accuracy: 0.9427\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 53us/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9740\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0084 - accuracy: 0.9987 - val_loss: 0.0811 - val_accuracy: 0.9688\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9792\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9688\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.1018 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0089 - accuracy: 0.9987 - val_loss: 0.0535 - val_accuracy: 0.9792\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 54us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9740\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9792\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9635\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9688\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 62us/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9844\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 42us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9740\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9740\n",
      "Epoch 31/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9792\n",
      "Epoch 32/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 0.9792\n",
      "Epoch 33/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9792\n",
      "479/479 [==============================] - 0s 188us/sample - loss: 0.1679 - accuracy: 0.9582\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 539us/sample - loss: 2.1993 - accuracy: 0.2180 - val_loss: 1.9949 - val_accuracy: 0.5938\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 56us/sample - loss: 1.4067 - accuracy: 0.7441 - val_loss: 0.9455 - val_accuracy: 0.6875\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 47us/sample - loss: 0.6323 - accuracy: 0.8068 - val_loss: 0.5332 - val_accuracy: 0.8594\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.3282 - accuracy: 0.9125 - val_loss: 0.3312 - val_accuracy: 0.9219\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.2250 - accuracy: 0.9504 - val_loss: 0.2770 - val_accuracy: 0.9062\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.1723 - accuracy: 0.9530 - val_loss: 0.2092 - val_accuracy: 0.9271\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.1455 - accuracy: 0.9648 - val_loss: 0.1651 - val_accuracy: 0.9427\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.1279 - accuracy: 0.9608 - val_loss: 0.1524 - val_accuracy: 0.9427\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0893 - accuracy: 0.9804 - val_loss: 0.1387 - val_accuracy: 0.9427\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0893 - accuracy: 0.9765 - val_loss: 0.1397 - val_accuracy: 0.9583\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 52us/sample - loss: 0.0928 - accuracy: 0.9674 - val_loss: 0.1747 - val_accuracy: 0.9479\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 54us/sample - loss: 0.0680 - accuracy: 0.9817 - val_loss: 0.1366 - val_accuracy: 0.9479\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 51us/sample - loss: 0.0680 - accuracy: 0.9791 - val_loss: 0.1426 - val_accuracy: 0.9635\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 37us/sample - loss: 0.0570 - accuracy: 0.9856 - val_loss: 0.1039 - val_accuracy: 0.9740\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0364 - accuracy: 0.9896 - val_loss: 0.1375 - val_accuracy: 0.9583\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0537 - accuracy: 0.9883 - val_loss: 0.1582 - val_accuracy: 0.9479\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 66us/sample - loss: 0.0432 - accuracy: 0.9909 - val_loss: 0.1366 - val_accuracy: 0.9479\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 47us/sample - loss: 0.0256 - accuracy: 0.9935 - val_loss: 0.1232 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 57us/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.1199 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 33us/sample - loss: 0.0152 - accuracy: 0.9987 - val_loss: 0.0831 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 0.9583\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0107 - accuracy: 0.9987 - val_loss: 0.1012 - val_accuracy: 0.9583\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0105 - accuracy: 0.9987 - val_loss: 0.1013 - val_accuracy: 0.9635\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0126 - accuracy: 0.9974 - val_loss: 0.0925 - val_accuracy: 0.9688\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0107 - accuracy: 0.9987 - val_loss: 0.0908 - val_accuracy: 0.9688\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.0966 - val_accuracy: 0.9635\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 50us/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9635\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 51us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9635\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9583\n",
      "479/479 [==============================] - 0s 161us/sample - loss: 0.1459 - accuracy: 0.9645\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 737us/sample - loss: 2.2097 - accuracy: 0.2572 - val_loss: 2.0298 - val_accuracy: 0.3854\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 1.5224 - accuracy: 0.6279 - val_loss: 0.8961 - val_accuracy: 0.7760\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 45us/sample - loss: 0.6444 - accuracy: 0.7924 - val_loss: 0.5686 - val_accuracy: 0.8177\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.3992 - accuracy: 0.8916 - val_loss: 0.4134 - val_accuracy: 0.8854\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.2544 - accuracy: 0.9178 - val_loss: 0.3109 - val_accuracy: 0.9167\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 52us/sample - loss: 0.1863 - accuracy: 0.9491 - val_loss: 0.3152 - val_accuracy: 0.9115\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 38us/sample - loss: 0.1575 - accuracy: 0.9504 - val_loss: 0.2865 - val_accuracy: 0.9062\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.1369 - accuracy: 0.9517 - val_loss: 0.2769 - val_accuracy: 0.9219\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 59us/sample - loss: 0.0980 - accuracy: 0.9791 - val_loss: 0.2049 - val_accuracy: 0.9375\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 0.0883 - accuracy: 0.9791 - val_loss: 0.2200 - val_accuracy: 0.9375\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 64us/sample - loss: 0.0702 - accuracy: 0.9817 - val_loss: 0.2207 - val_accuracy: 0.9323\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 64us/sample - loss: 0.0566 - accuracy: 0.9843 - val_loss: 0.2409 - val_accuracy: 0.9375\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 44us/sample - loss: 0.0549 - accuracy: 0.9909 - val_loss: 0.2303 - val_accuracy: 0.9583\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 44us/sample - loss: 0.0459 - accuracy: 0.9922 - val_loss: 0.1640 - val_accuracy: 0.9479\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 48us/sample - loss: 0.0375 - accuracy: 0.9948 - val_loss: 0.1820 - val_accuracy: 0.9531\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 51us/sample - loss: 0.0249 - accuracy: 0.9974 - val_loss: 0.1905 - val_accuracy: 0.9427\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 48us/sample - loss: 0.0384 - accuracy: 0.9935 - val_loss: 0.2473 - val_accuracy: 0.9323\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 52us/sample - loss: 0.0343 - accuracy: 0.9922 - val_loss: 0.2167 - val_accuracy: 0.9375\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 52us/sample - loss: 0.0278 - accuracy: 0.9948 - val_loss: 0.1808 - val_accuracy: 0.9479\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9583\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.9479\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.1988 - val_accuracy: 0.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 54us/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9635\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 39us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1808 - val_accuracy: 0.9583\n",
      "479/479 [==============================] - 0s 163us/sample - loss: 0.1380 - accuracy: 0.9582\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 508us/sample - loss: 1.4764 - accuracy: 0.5888 - val_loss: 0.4616 - val_accuracy: 0.8438\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 0.3426 - accuracy: 0.8903 - val_loss: 0.1937 - val_accuracy: 0.9583\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 57us/sample - loss: 0.1324 - accuracy: 0.9582 - val_loss: 0.1374 - val_accuracy: 0.9583\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 59us/sample - loss: 0.1073 - accuracy: 0.9661 - val_loss: 0.1100 - val_accuracy: 0.9635\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0744 - accuracy: 0.9765 - val_loss: 0.1053 - val_accuracy: 0.9740\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0416 - accuracy: 0.9922 - val_loss: 0.0979 - val_accuracy: 0.9688\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0339 - accuracy: 0.9922 - val_loss: 0.1079 - val_accuracy: 0.9583\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0213 - accuracy: 0.9974 - val_loss: 0.0805 - val_accuracy: 0.9531\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.0808 - val_accuracy: 0.9688\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.0673 - val_accuracy: 0.9688\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.0556 - val_accuracy: 0.9740\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9635\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 56us/sample - loss: 0.0129 - accuracy: 0.9974 - val_loss: 0.0722 - val_accuracy: 0.9740\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 62us/sample - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.1123 - val_accuracy: 0.9479\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 54us/sample - loss: 0.0224 - accuracy: 0.9948 - val_loss: 0.0908 - val_accuracy: 0.9688\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 55us/sample - loss: 0.0364 - accuracy: 0.9869 - val_loss: 0.1121 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0357 - accuracy: 0.9896 - val_loss: 0.0919 - val_accuracy: 0.9688\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 0.0347 - accuracy: 0.9909 - val_loss: 0.0977 - val_accuracy: 0.9531\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.1000 - val_accuracy: 0.9583\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.0821 - val_accuracy: 0.9688\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9740\n",
      "479/479 [==============================] - 0s 163us/sample - loss: 0.2411 - accuracy: 0.9457\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 486us/sample - loss: 1.3559 - accuracy: 0.6658 - val_loss: 0.4555 - val_accuracy: 0.8646\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.3635 - accuracy: 0.8838 - val_loss: 0.5056 - val_accuracy: 0.8490\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.2148 - accuracy: 0.9295 - val_loss: 0.2420 - val_accuracy: 0.9375\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.1188 - accuracy: 0.9621 - val_loss: 0.1453 - val_accuracy: 0.9479\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0816 - accuracy: 0.9752 - val_loss: 0.1547 - val_accuracy: 0.9375\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 51us/sample - loss: 0.0447 - accuracy: 0.9909 - val_loss: 0.0848 - val_accuracy: 0.9688\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0385 - accuracy: 0.9896 - val_loss: 0.0994 - val_accuracy: 0.9740\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 70us/sample - loss: 0.0406 - accuracy: 0.9869 - val_loss: 0.0924 - val_accuracy: 0.9635\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 43us/sample - loss: 0.0288 - accuracy: 0.9948 - val_loss: 0.0816 - val_accuracy: 0.9740\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0216 - accuracy: 0.9935 - val_loss: 0.1289 - val_accuracy: 0.9531\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.1147 - val_accuracy: 0.9531\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.0831 - val_accuracy: 0.9792\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0185 - accuracy: 0.9961 - val_loss: 0.1106 - val_accuracy: 0.9531\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0270 - accuracy: 0.9935 - val_loss: 0.1359 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 0.0486 - accuracy: 0.9869 - val_loss: 0.1605 - val_accuracy: 0.9427\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 64us/sample - loss: 0.0791 - accuracy: 0.9765 - val_loss: 0.1974 - val_accuracy: 0.9375\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 50us/sample - loss: 0.0260 - accuracy: 0.9935 - val_loss: 0.0972 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 0.9688\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9688\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9688\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 8.1085e-04 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9688\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 6.8364e-04 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9688\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 5.6172e-04 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9688\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 41us/sample - loss: 4.8471e-04 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9688\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 51us/sample - loss: 4.2674e-04 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9688\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 3.8996e-04 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9688\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 70us/sample - loss: 3.5174e-04 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9688\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 3.1763e-04 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9688\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 2.8250e-04 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 0.9688\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 72us/sample - loss: 2.6284e-04 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9688\n",
      "Epoch 31/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 70us/sample - loss: 2.3951e-04 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 0.9688\n",
      "Epoch 32/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 2.3841e-04 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9688\n",
      "Epoch 33/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 2.1349e-04 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9688\n",
      "Epoch 34/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 2.0191e-04 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 0.9688\n",
      "Epoch 35/2000\n",
      "766/766 [==============================] - 0s 44us/sample - loss: 1.8612e-04 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9688\n",
      "Epoch 36/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 1.7853e-04 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9688\n",
      "Epoch 37/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 1.6702e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9688\n",
      "479/479 [==============================] - 0s 131us/sample - loss: 0.1223 - accuracy: 0.9645\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 494us/sample - loss: 1.3743 - accuracy: 0.6554 - val_loss: 0.4724 - val_accuracy: 0.8750\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 45us/sample - loss: 0.3661 - accuracy: 0.8773 - val_loss: 0.3159 - val_accuracy: 0.8958\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 74us/sample - loss: 0.2042 - accuracy: 0.9295 - val_loss: 0.2996 - val_accuracy: 0.9010\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 44us/sample - loss: 0.1716 - accuracy: 0.9426 - val_loss: 0.3124 - val_accuracy: 0.9010\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 83us/sample - loss: 0.1093 - accuracy: 0.9687 - val_loss: 0.1636 - val_accuracy: 0.9531\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 0.0564 - accuracy: 0.9830 - val_loss: 0.1914 - val_accuracy: 0.9375\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0539 - accuracy: 0.9856 - val_loss: 0.2319 - val_accuracy: 0.9375\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.1985 - val_accuracy: 0.9479\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0169 - accuracy: 0.9974 - val_loss: 0.1506 - val_accuracy: 0.9583\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 64us/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9531\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9583\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1399 - val_accuracy: 0.9635\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1359 - val_accuracy: 0.9635\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1408 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9635\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1403 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 83us/sample - loss: 9.8081e-04 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 49us/sample - loss: 8.5650e-04 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 7.8175e-04 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - ETA: 0s - loss: 8.6691e-04 - accuracy: 1.00 - 0s 61us/sample - loss: 7.2840e-04 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 6.3743e-04 - accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 163us/sample - loss: 0.0851 - accuracy: 0.9708\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 873us/sample - loss: 1.1430 - accuracy: 0.6031 - val_loss: 0.4803 - val_accuracy: 0.8281\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 237us/sample - loss: 0.3257 - accuracy: 0.8890 - val_loss: 0.2502 - val_accuracy: 0.9375\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 204us/sample - loss: 0.1643 - accuracy: 0.9478 - val_loss: 0.1307 - val_accuracy: 0.9531\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 224us/sample - loss: 0.1424 - accuracy: 0.9621 - val_loss: 0.2428 - val_accuracy: 0.9375\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 231us/sample - loss: 0.0857 - accuracy: 0.9778 - val_loss: 0.1278 - val_accuracy: 0.9740\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 204us/sample - loss: 0.1493 - accuracy: 0.9504 - val_loss: 0.2131 - val_accuracy: 0.9271\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 216us/sample - loss: 0.0955 - accuracy: 0.9713 - val_loss: 0.0982 - val_accuracy: 0.9635\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 215us/sample - loss: 0.1415 - accuracy: 0.9634 - val_loss: 0.3239 - val_accuracy: 0.9323\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 204us/sample - loss: 0.1818 - accuracy: 0.9556 - val_loss: 0.3684 - val_accuracy: 0.8906\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 224us/sample - loss: 0.1021 - accuracy: 0.9713 - val_loss: 0.3547 - val_accuracy: 0.9375\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 204us/sample - loss: 0.1449 - accuracy: 0.9648 - val_loss: 0.0932 - val_accuracy: 0.9688\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 236us/sample - loss: 0.0483 - accuracy: 0.9896 - val_loss: 0.2981 - val_accuracy: 0.9479\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 224us/sample - loss: 0.0622 - accuracy: 0.9778 - val_loss: 0.2941 - val_accuracy: 0.9479\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 212us/sample - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.1806 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 228us/sample - loss: 0.0335 - accuracy: 0.9922 - val_loss: 0.4516 - val_accuracy: 0.9271\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 201us/sample - loss: 0.0941 - accuracy: 0.9739 - val_loss: 0.2564 - val_accuracy: 0.9375\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 226us/sample - loss: 0.1077 - accuracy: 0.9700 - val_loss: 0.1531 - val_accuracy: 0.9479\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 204us/sample - loss: 0.1342 - accuracy: 0.9595 - val_loss: 0.2027 - val_accuracy: 0.9323\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 199us/sample - loss: 0.0682 - accuracy: 0.9830 - val_loss: 0.1522 - val_accuracy: 0.9427\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 216us/sample - loss: 0.2061 - accuracy: 0.9595 - val_loss: 0.2743 - val_accuracy: 0.9375\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 200us/sample - loss: 0.0935 - accuracy: 0.9713 - val_loss: 0.4436 - val_accuracy: 0.9167\n",
      "479/479 [==============================] - 0s 163us/sample - loss: 0.7304 - accuracy: 0.9165\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 677us/sample - loss: 1.1867 - accuracy: 0.6070 - val_loss: 0.3144 - val_accuracy: 0.9271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 246us/sample - loss: 0.2527 - accuracy: 0.9204 - val_loss: 0.4545 - val_accuracy: 0.8750\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 252us/sample - loss: 0.1540 - accuracy: 0.9504 - val_loss: 0.3344 - val_accuracy: 0.9010\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 230us/sample - loss: 0.1642 - accuracy: 0.9465 - val_loss: 0.2494 - val_accuracy: 0.9323\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 232us/sample - loss: 0.1391 - accuracy: 0.9582 - val_loss: 0.2904 - val_accuracy: 0.9219\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 225us/sample - loss: 0.1065 - accuracy: 0.9634 - val_loss: 0.2542 - val_accuracy: 0.9219\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 223us/sample - loss: 0.1847 - accuracy: 0.9439 - val_loss: 0.2848 - val_accuracy: 0.9427\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 248us/sample - loss: 0.1025 - accuracy: 0.9621 - val_loss: 0.1647 - val_accuracy: 0.9635\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 260us/sample - loss: 0.0855 - accuracy: 0.9739 - val_loss: 0.1684 - val_accuracy: 0.9479\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 243us/sample - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.4862 - val_accuracy: 0.9271\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 223us/sample - loss: 0.1585 - accuracy: 0.9582 - val_loss: 0.4181 - val_accuracy: 0.8958\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 228us/sample - loss: 0.1817 - accuracy: 0.9439 - val_loss: 0.1414 - val_accuracy: 0.9427\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 231us/sample - loss: 0.1446 - accuracy: 0.9674 - val_loss: 0.1548 - val_accuracy: 0.9375\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 244us/sample - loss: 0.1007 - accuracy: 0.9726 - val_loss: 0.2057 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 231us/sample - loss: 0.1009 - accuracy: 0.9804 - val_loss: 0.3528 - val_accuracy: 0.9010\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 238us/sample - loss: 0.1817 - accuracy: 0.9595 - val_loss: 0.3900 - val_accuracy: 0.9219\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 235us/sample - loss: 0.1753 - accuracy: 0.9634 - val_loss: 0.1567 - val_accuracy: 0.9688\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 231us/sample - loss: 0.1039 - accuracy: 0.9752 - val_loss: 0.1540 - val_accuracy: 0.9427\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 258us/sample - loss: 0.0346 - accuracy: 0.9948 - val_loss: 0.1093 - val_accuracy: 0.9740\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 256us/sample - loss: 0.1166 - accuracy: 0.9726 - val_loss: 0.2276 - val_accuracy: 0.9323\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 240us/sample - loss: 0.0880 - accuracy: 0.9843 - val_loss: 0.0704 - val_accuracy: 0.9583\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 242us/sample - loss: 0.1809 - accuracy: 0.9634 - val_loss: 0.1249 - val_accuracy: 0.9740\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 242us/sample - loss: 0.0657 - accuracy: 0.9739 - val_loss: 0.0889 - val_accuracy: 0.9740\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 240us/sample - loss: 0.1228 - accuracy: 0.9661 - val_loss: 0.5665 - val_accuracy: 0.8438\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 253us/sample - loss: 0.1469 - accuracy: 0.9595 - val_loss: 0.4982 - val_accuracy: 0.9219\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 300us/sample - loss: 0.1354 - accuracy: 0.9230 - val_loss: 0.4772 - val_accuracy: 0.8542\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 290us/sample - loss: 0.1635 - accuracy: 0.8708 - val_loss: 0.5137 - val_accuracy: 0.8385\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 256us/sample - loss: 0.3621 - accuracy: 0.8499 - val_loss: 0.6071 - val_accuracy: 0.7500\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 227us/sample - loss: 0.2877 - accuracy: 0.7963 - val_loss: 0.5468 - val_accuracy: 0.7396\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 237us/sample - loss: 0.5343 - accuracy: 0.7428 - val_loss: 0.7130 - val_accuracy: 0.6406\n",
      "Epoch 31/2000\n",
      "766/766 [==============================] - 0s 238us/sample - loss: 0.5159 - accuracy: 0.6789 - val_loss: 0.6407 - val_accuracy: 0.5990\n",
      "479/479 [==============================] - 0s 198us/sample - loss: 0.8580 - accuracy: 0.6033\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - ETA: 0s - loss: 1.2169 - accuracy: 0.63 - 1s 689us/sample - loss: 1.1552 - accuracy: 0.6527 - val_loss: 0.5775 - val_accuracy: 0.8438\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 256us/sample - loss: 0.4773 - accuracy: 0.8590 - val_loss: 0.4274 - val_accuracy: 0.8802\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 253us/sample - loss: 0.1700 - accuracy: 0.9517 - val_loss: 0.3055 - val_accuracy: 0.9010\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 262us/sample - loss: 0.1703 - accuracy: 0.9478 - val_loss: 0.1848 - val_accuracy: 0.9479\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 251us/sample - loss: 0.1328 - accuracy: 0.9621 - val_loss: 0.2806 - val_accuracy: 0.9479\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 256us/sample - loss: 0.0776 - accuracy: 0.9765 - val_loss: 0.3179 - val_accuracy: 0.9479\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 328us/sample - loss: 0.1447 - accuracy: 0.9634 - val_loss: 0.3950 - val_accuracy: 0.9323\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 283us/sample - loss: 0.1010 - accuracy: 0.9700 - val_loss: 0.4685 - val_accuracy: 0.9323\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 290us/sample - loss: 0.0698 - accuracy: 0.9804 - val_loss: 0.3261 - val_accuracy: 0.9115\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 261us/sample - loss: 0.1410 - accuracy: 0.9595 - val_loss: 0.3855 - val_accuracy: 0.9115\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 242us/sample - loss: 0.1178 - accuracy: 0.9661 - val_loss: 0.4226 - val_accuracy: 0.9375\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 235us/sample - loss: 0.1527 - accuracy: 0.9556 - val_loss: 0.3012 - val_accuracy: 0.9427\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 226us/sample - loss: 0.1143 - accuracy: 0.9543 - val_loss: 0.1594 - val_accuracy: 0.9531\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 239us/sample - loss: 0.0906 - accuracy: 0.9700 - val_loss: 0.3577 - val_accuracy: 0.9062\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 237us/sample - loss: 0.1567 - accuracy: 0.9608 - val_loss: 0.2369 - val_accuracy: 0.9375\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 236us/sample - loss: 0.1360 - accuracy: 0.9726 - val_loss: 0.5395 - val_accuracy: 0.9219\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 213us/sample - loss: 0.1555 - accuracy: 0.9595 - val_loss: 0.2380 - val_accuracy: 0.9323\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 217us/sample - loss: 0.1079 - accuracy: 0.9674 - val_loss: 0.3446 - val_accuracy: 0.9479\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 288us/sample - loss: 0.1180 - accuracy: 0.9700 - val_loss: 0.2628 - val_accuracy: 0.9323\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 234us/sample - loss: 0.2161 - accuracy: 0.9543 - val_loss: 0.5757 - val_accuracy: 0.8958\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 228us/sample - loss: 0.0715 - accuracy: 0.9804 - val_loss: 0.3355 - val_accuracy: 0.9271\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 240us/sample - loss: 0.0777 - accuracy: 0.9804 - val_loss: 0.5148 - val_accuracy: 0.9531\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 227us/sample - loss: 0.1859 - accuracy: 0.9726 - val_loss: 0.5956 - val_accuracy: 0.9010\n",
      "479/479 [==============================] - 0s 196us/sample - loss: 0.4969 - accuracy: 0.9081\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 1s 830us/sample - loss: 1.9699 - accuracy: 0.2755 - val_loss: 1.4457 - val_accuracy: 0.3750\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 146us/sample - loss: 1.0964 - accuracy: 0.5627 - val_loss: 0.9225 - val_accuracy: 0.6875\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 136us/sample - loss: 0.7008 - accuracy: 0.7428 - val_loss: 0.8012 - val_accuracy: 0.7083\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.5888 - accuracy: 0.7781 - val_loss: 0.5891 - val_accuracy: 0.7656\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 148us/sample - loss: 0.3744 - accuracy: 0.8460 - val_loss: 0.3595 - val_accuracy: 0.8333\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 115us/sample - loss: 0.4039 - accuracy: 0.8264 - val_loss: 0.4480 - val_accuracy: 0.8438\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 88us/sample - loss: 0.3815 - accuracy: 0.8486 - val_loss: 0.4782 - val_accuracy: 0.7917\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.3298 - accuracy: 0.8655 - val_loss: 0.4807 - val_accuracy: 0.8073\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.3635 - accuracy: 0.8760 - val_loss: 0.5027 - val_accuracy: 0.8333\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.2239 - accuracy: 0.8747 - val_loss: 0.5369 - val_accuracy: 0.8385\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.2160 - accuracy: 0.8603 - val_loss: 0.4777 - val_accuracy: 0.8333\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 115us/sample - loss: 0.2482 - accuracy: 0.8655 - val_loss: 0.4442 - val_accuracy: 0.8073\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 111us/sample - loss: 0.2420 - accuracy: 0.8629 - val_loss: 0.6754 - val_accuracy: 0.7917\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 112us/sample - loss: 0.4804 - accuracy: 0.8381 - val_loss: 0.7646 - val_accuracy: 0.7760\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 111us/sample - loss: 0.5824 - accuracy: 0.7624 - val_loss: 0.3961 - val_accuracy: 0.8229\n",
      "479/479 [==============================] - 0s 190us/sample - loss: 0.5711 - accuracy: 0.8309\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 670us/sample - loss: 1.6944 - accuracy: 0.3642 - val_loss: 1.4549 - val_accuracy: 0.5885\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 158us/sample - loss: 0.8640 - accuracy: 0.6762 - val_loss: 0.8787 - val_accuracy: 0.7500\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 158us/sample - loss: 0.7691 - accuracy: 0.7245 - val_loss: 0.6243 - val_accuracy: 0.7031\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 158us/sample - loss: 0.6897 - accuracy: 0.7702 - val_loss: 0.5820 - val_accuracy: 0.8385\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 138us/sample - loss: 0.4234 - accuracy: 0.8760 - val_loss: 0.7279 - val_accuracy: 0.8438\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 136us/sample - loss: 0.2996 - accuracy: 0.9178 - val_loss: 0.5447 - val_accuracy: 0.8333\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 131us/sample - loss: 0.2881 - accuracy: 0.9047 - val_loss: 0.3661 - val_accuracy: 0.9219\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 124us/sample - loss: 0.2390 - accuracy: 0.9347 - val_loss: 0.3007 - val_accuracy: 0.9167\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 123us/sample - loss: 0.1669 - accuracy: 0.9504 - val_loss: 0.2192 - val_accuracy: 0.9531\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 125us/sample - loss: 0.8615 - accuracy: 0.8629 - val_loss: 0.4064 - val_accuracy: 0.8906\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 124us/sample - loss: 0.3425 - accuracy: 0.9321 - val_loss: 0.4555 - val_accuracy: 0.9375\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 120us/sample - loss: 0.2644 - accuracy: 0.9413 - val_loss: 0.3285 - val_accuracy: 0.9375\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 129us/sample - loss: 0.0897 - accuracy: 0.9765 - val_loss: 0.1190 - val_accuracy: 0.9635\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 123us/sample - loss: 0.0920 - accuracy: 0.9765 - val_loss: 0.4936 - val_accuracy: 0.9427\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 125us/sample - loss: 0.1529 - accuracy: 0.9726 - val_loss: 0.6502 - val_accuracy: 0.8802\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 116us/sample - loss: 0.1537 - accuracy: 0.9621 - val_loss: 0.3094 - val_accuracy: 0.9531\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 117us/sample - loss: 0.0527 - accuracy: 0.9830 - val_loss: 0.2094 - val_accuracy: 0.9583\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 105us/sample - loss: 0.0460 - accuracy: 0.9922 - val_loss: 0.2254 - val_accuracy: 0.9583\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 111us/sample - loss: 0.0527 - accuracy: 0.9856 - val_loss: 0.2380 - val_accuracy: 0.9583\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 108us/sample - loss: 0.5610 - accuracy: 0.9230 - val_loss: 1.1816 - val_accuracy: 0.8438\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 0.6182 - accuracy: 0.8956 - val_loss: 1.8553 - val_accuracy: 0.7812\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 96us/sample - loss: 0.9071 - accuracy: 0.7089 - val_loss: 0.8074 - val_accuracy: 0.5625\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 96us/sample - loss: 1.0631 - accuracy: 0.5849 - val_loss: 0.8260 - val_accuracy: 0.5208\n",
      "479/479 [==============================] - 0s 163us/sample - loss: 0.9005 - accuracy: 0.5616\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 629us/sample - loss: 1.7198 - accuracy: 0.3512 - val_loss: 1.2284 - val_accuracy: 0.5156\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 143us/sample - loss: 0.9442 - accuracy: 0.6070 - val_loss: 0.8738 - val_accuracy: 0.6406\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 143us/sample - loss: 0.7394 - accuracy: 0.6984 - val_loss: 0.8828 - val_accuracy: 0.7865\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 156us/sample - loss: 0.5308 - accuracy: 0.7663 - val_loss: 0.8632 - val_accuracy: 0.7865\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 143us/sample - loss: 0.5332 - accuracy: 0.7572 - val_loss: 0.5545 - val_accuracy: 0.8021\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.5660 - accuracy: 0.8133 - val_loss: 0.9877 - val_accuracy: 0.7292\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 147us/sample - loss: 0.5645 - accuracy: 0.7937 - val_loss: 0.5141 - val_accuracy: 0.7812\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 123us/sample - loss: 0.4341 - accuracy: 0.8394 - val_loss: 0.4836 - val_accuracy: 0.8281\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 0.3504 - accuracy: 0.8499 - val_loss: 0.3268 - val_accuracy: 0.8438\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.3659 - accuracy: 0.8903 - val_loss: 0.4136 - val_accuracy: 0.8125\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 1.0576 - accuracy: 0.7520 - val_loss: 1.6855 - val_accuracy: 0.7552\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.6480 - accuracy: 0.7611 - val_loss: 0.6366 - val_accuracy: 0.7969\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.5439 - accuracy: 0.7206 - val_loss: 0.6752 - val_accuracy: 0.6406\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.5667 - accuracy: 0.6710 - val_loss: 0.7445 - val_accuracy: 0.7135\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.5619 - accuracy: 0.7050 - val_loss: 0.9501 - val_accuracy: 0.7448\n",
      "Epoch 16/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 126us/sample - loss: 1.3741 - accuracy: 0.6358 - val_loss: 2.4627 - val_accuracy: 0.5469\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 132us/sample - loss: 0.9814 - accuracy: 0.6606 - val_loss: 1.0371 - val_accuracy: 0.6875\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 122us/sample - loss: 0.6433 - accuracy: 0.6501 - val_loss: 0.7878 - val_accuracy: 0.6927\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 117us/sample - loss: 0.5207 - accuracy: 0.7585 - val_loss: 0.5601 - val_accuracy: 0.7552\n",
      "479/479 [==============================] - 0s 212us/sample - loss: 0.5859 - accuracy: 0.7370\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 494us/sample - loss: 1.4061 - accuracy: 0.6044 - val_loss: 0.4639 - val_accuracy: 0.8281\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.3466 - accuracy: 0.8916 - val_loss: 0.2424 - val_accuracy: 0.9219\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.1411 - accuracy: 0.9556 - val_loss: 0.1503 - val_accuracy: 0.9531\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.1207 - accuracy: 0.9530 - val_loss: 0.1244 - val_accuracy: 0.9583\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.0696 - accuracy: 0.9830 - val_loss: 0.0882 - val_accuracy: 0.9740\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0327 - accuracy: 0.9961 - val_loss: 0.0816 - val_accuracy: 0.9635\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.0264 - accuracy: 0.9935 - val_loss: 0.1428 - val_accuracy: 0.9323\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 0.0189 - accuracy: 0.9987 - val_loss: 0.0607 - val_accuracy: 0.9740\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 60us/sample - loss: 0.0202 - accuracy: 0.9961 - val_loss: 0.0822 - val_accuracy: 0.9688\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 83us/sample - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0664 - val_accuracy: 0.9792\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0693 - accuracy: 0.9739 - val_loss: 0.1424 - val_accuracy: 0.9375\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 0.0240 - accuracy: 0.9961 - val_loss: 0.1512 - val_accuracy: 0.9427\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 66us/sample - loss: 0.0094 - accuracy: 0.9987 - val_loss: 0.1169 - val_accuracy: 0.9583\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9688\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9792\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 84us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9740\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 9.2292e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9688\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 8.1262e-04 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9740\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 6.7818e-04 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9740\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 6.0797e-04 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9740\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 85us/sample - loss: 5.3306e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9740\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 4.6256e-04 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9740\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 4.2752e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9740\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 3.8834e-04 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9740\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 3.5158e-04 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9688\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 86us/sample - loss: 3.2081e-04 - accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 0.9688\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 2.9889e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9688\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 79us/sample - loss: 2.7209e-04 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 0.9740\n",
      "479/479 [==============================] - 0s 159us/sample - loss: 0.1985 - accuracy: 0.9666\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 501us/sample - loss: 1.3558 - accuracy: 0.6397 - val_loss: 0.5066 - val_accuracy: 0.8333\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.3177 - accuracy: 0.9060 - val_loss: 0.3887 - val_accuracy: 0.8906\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.2536 - accuracy: 0.9178 - val_loss: 0.2647 - val_accuracy: 0.9115\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.1484 - accuracy: 0.9595 - val_loss: 0.1482 - val_accuracy: 0.9479\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.0904 - accuracy: 0.9726 - val_loss: 0.1634 - val_accuracy: 0.9427\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 0.0590 - accuracy: 0.9817 - val_loss: 0.0978 - val_accuracy: 0.9583\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.1637 - val_accuracy: 0.9375\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.0368 - accuracy: 0.9909 - val_loss: 0.0933 - val_accuracy: 0.9688\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 0.0242 - accuracy: 0.9935 - val_loss: 0.0714 - val_accuracy: 0.9740\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.1208 - val_accuracy: 0.9635\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.1022 - val_accuracy: 0.9583\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0175 - accuracy: 0.9987 - val_loss: 0.0936 - val_accuracy: 0.9740\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0281 - accuracy: 0.9935 - val_loss: 0.2520 - val_accuracy: 0.9219\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 0.1013 - accuracy: 0.9661 - val_loss: 0.1042 - val_accuracy: 0.9688\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 0.0436 - accuracy: 0.9896 - val_loss: 0.1232 - val_accuracy: 0.9531\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 66us/sample - loss: 0.0170 - accuracy: 0.9948 - val_loss: 0.0865 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9740\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9688\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9688\n",
      "479/479 [==============================] - 0s 167us/sample - loss: 0.1088 - accuracy: 0.9687\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 521us/sample - loss: 1.3207 - accuracy: 0.6606 - val_loss: 0.4466 - val_accuracy: 0.8385\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 68us/sample - loss: 0.3318 - accuracy: 0.8956 - val_loss: 0.3345 - val_accuracy: 0.8750\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.1772 - accuracy: 0.9504 - val_loss: 0.2514 - val_accuracy: 0.9271\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 92us/sample - loss: 0.1490 - accuracy: 0.9426 - val_loss: 0.3102 - val_accuracy: 0.9010\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 61us/sample - loss: 0.0926 - accuracy: 0.9752 - val_loss: 0.1815 - val_accuracy: 0.9583\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 86us/sample - loss: 0.0546 - accuracy: 0.9817 - val_loss: 0.1747 - val_accuracy: 0.9479\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 72us/sample - loss: 0.0296 - accuracy: 0.9935 - val_loss: 0.1921 - val_accuracy: 0.9427\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0458 - accuracy: 0.9869 - val_loss: 0.1828 - val_accuracy: 0.9531\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0230 - accuracy: 0.9948 - val_loss: 0.1774 - val_accuracy: 0.9583\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9583\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 70us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9583\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1460 - val_accuracy: 0.9635\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9635\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 75us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9635\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9635\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1516 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 70us/sample - loss: 9.9952e-04 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 9.2848e-04 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 182us/sample - loss: 0.0976 - accuracy: 0.9645\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 841us/sample - loss: 8.6844 - accuracy: 0.1044 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 347us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 295us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 313us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 306us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 306us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 353us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 302us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 316us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 347us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 306us/sample - loss: 8.8069 - accuracy: 0.1031 - val_loss: 8.7882 - val_accuracy: 0.0781\n",
      "479/479 [==============================] - 0s 222us/sample - loss: 8.8183 - accuracy: 0.1106\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 1ms/sample - loss: 8.6656 - accuracy: 0.1084 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 368us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 339us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 324us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 328us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 321us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 330us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 360us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 322us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 369us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 320us/sample - loss: 8.8448 - accuracy: 0.1123 - val_loss: 8.7126 - val_accuracy: 0.0781\n",
      "479/479 [==============================] - 0s 246us/sample - loss: 8.9698 - accuracy: 0.0960\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 869us/sample - loss: 10.0014 - accuracy: 0.1057 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 350us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 337us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 332us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 306us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 341us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 312us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 326us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 326us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 10/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 326us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 320us/sample - loss: 10.2908 - accuracy: 0.1070 - val_loss: 10.0566 - val_accuracy: 0.0729\n",
      "479/479 [==============================] - 0s 241us/sample - loss: 10.3053 - accuracy: 0.0960\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 609us/sample - loss: 1.4397 - accuracy: 0.5561 - val_loss: 0.9733 - val_accuracy: 0.6927\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 0.4667 - accuracy: 0.8446 - val_loss: 0.4415 - val_accuracy: 0.8542\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 0.1951 - accuracy: 0.9282 - val_loss: 0.2133 - val_accuracy: 0.9375\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.1487 - accuracy: 0.9413 - val_loss: 0.1741 - val_accuracy: 0.9375\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 0.1119 - accuracy: 0.9700 - val_loss: 0.1885 - val_accuracy: 0.9323\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.0937 - val_accuracy: 0.9688\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0400 - accuracy: 0.9896 - val_loss: 0.2508 - val_accuracy: 0.9323\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.0650 - accuracy: 0.9752 - val_loss: 0.1142 - val_accuracy: 0.9479\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.0217 - accuracy: 0.9948 - val_loss: 0.0890 - val_accuracy: 0.9740\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.1413 - val_accuracy: 0.9635\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9792\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9844\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 8.2094e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9688\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 3.7105e-04 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9740\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 2.5091e-04 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9740\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 2.0487e-04 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9740\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 1.7372e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9740\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 84us/sample - loss: 1.4978e-04 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9740\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 1.3297e-04 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9740\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 1.2435e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9740\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 106us/sample - loss: 1.0951e-04 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9740\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 9.9301e-05 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9740\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 9.0345e-05 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9740\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 8.3052e-05 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9792\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 7.7040e-05 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9792\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 6.9957e-05 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9740\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 6.4685e-05 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9740\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 84us/sample - loss: 5.9942e-05 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9740\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 5.5131e-05 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9740\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 5.1159e-05 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9792\n",
      "479/479 [==============================] - 0s 186us/sample - loss: 0.2679 - accuracy: 0.9603\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 592us/sample - loss: 1.4010 - accuracy: 0.5614 - val_loss: 0.5634 - val_accuracy: 0.8125\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 128us/sample - loss: 0.4144 - accuracy: 0.8747 - val_loss: 0.2594 - val_accuracy: 0.9062\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 0.1509 - accuracy: 0.9517 - val_loss: 0.3092 - val_accuracy: 0.8958\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.1025 - accuracy: 0.9687 - val_loss: 0.1149 - val_accuracy: 0.9427\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 0.0843 - accuracy: 0.9713 - val_loss: 0.2493 - val_accuracy: 0.9323\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 101us/sample - loss: 0.0484 - accuracy: 0.9843 - val_loss: 0.0819 - val_accuracy: 0.9635\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 0.0504 - accuracy: 0.9804 - val_loss: 0.1929 - val_accuracy: 0.9219\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.0643 - accuracy: 0.9765 - val_loss: 0.1401 - val_accuracy: 0.9479\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.1394 - val_accuracy: 0.9635\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 106us/sample - loss: 0.0225 - accuracy: 0.9922 - val_loss: 0.1552 - val_accuracy: 0.9479\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.2582 - val_accuracy: 0.9271\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.1371 - val_accuracy: 0.9479\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 100us/sample - loss: 0.0293 - accuracy: 0.9896 - val_loss: 0.0773 - val_accuracy: 0.9688\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 98us/sample - loss: 0.0125 - accuracy: 0.9974 - val_loss: 0.1005 - val_accuracy: 0.9583\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9688\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 89us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9688\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 4.0832e-04 - accuracy: 1.0000 - val_loss: 0.1217 - val_accuracy: 0.9635\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 77us/sample - loss: 1.8342e-04 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 9.4797e-05 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 85us/sample - loss: 7.0675e-05 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 5.7504e-05 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 4.8887e-05 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 4.1195e-05 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 184us/sample - loss: 0.1420 - accuracy: 0.9749\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 600us/sample - loss: 1.4126 - accuracy: 0.5679 - val_loss: 0.6577 - val_accuracy: 0.8073\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 97us/sample - loss: 0.4497 - accuracy: 0.8603 - val_loss: 0.3384 - val_accuracy: 0.8646\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.2505 - accuracy: 0.9269 - val_loss: 0.3154 - val_accuracy: 0.8906\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 112us/sample - loss: 0.1596 - accuracy: 0.9452 - val_loss: 0.2816 - val_accuracy: 0.9115\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 115us/sample - loss: 0.0818 - accuracy: 0.9739 - val_loss: 0.1713 - val_accuracy: 0.9427\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 0.0539 - accuracy: 0.9804 - val_loss: 0.1959 - val_accuracy: 0.9479\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 0.0765 - accuracy: 0.9687 - val_loss: 0.2067 - val_accuracy: 0.9323\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 111us/sample - loss: 0.1072 - accuracy: 0.9621 - val_loss: 0.3126 - val_accuracy: 0.9167\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 108us/sample - loss: 0.0365 - accuracy: 0.9883 - val_loss: 0.1688 - val_accuracy: 0.9583\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 108us/sample - loss: 0.0272 - accuracy: 0.9896 - val_loss: 0.2286 - val_accuracy: 0.9427\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.0757 - val_accuracy: 0.9792\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 107us/sample - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.1140 - val_accuracy: 0.9688\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 110us/sample - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.1130 - val_accuracy: 0.9635\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 115us/sample - loss: 0.0140 - accuracy: 0.9961 - val_loss: 0.2028 - val_accuracy: 0.9323\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 93us/sample - loss: 0.0782 - accuracy: 0.9739 - val_loss: 0.2009 - val_accuracy: 0.9323\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 90us/sample - loss: 0.0362 - accuracy: 0.9935 - val_loss: 0.2033 - val_accuracy: 0.9583\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 0.0274 - accuracy: 0.9935 - val_loss: 0.1840 - val_accuracy: 0.9427\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.1241 - val_accuracy: 0.9635\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.0261 - accuracy: 0.9896 - val_loss: 0.1177 - val_accuracy: 0.9688\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 102us/sample - loss: 0.0338 - accuracy: 0.9896 - val_loss: 0.2073 - val_accuracy: 0.9479\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 103us/sample - loss: 0.0151 - accuracy: 0.9974 - val_loss: 0.0650 - val_accuracy: 0.9740\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 106us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 94us/sample - loss: 1.8309e-04 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9740\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 5.5133e-05 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9740\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 95us/sample - loss: 3.8571e-05 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9740\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 3.0724e-05 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9740\n",
      "Epoch 27/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 2.4910e-05 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9740\n",
      "Epoch 28/2000\n",
      "766/766 [==============================] - 0s 104us/sample - loss: 2.1213e-05 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9740\n",
      "Epoch 29/2000\n",
      "766/766 [==============================] - 0s 99us/sample - loss: 1.7970e-05 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9740\n",
      "Epoch 30/2000\n",
      "766/766 [==============================] - 0s 108us/sample - loss: 1.5888e-05 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9740\n",
      "Epoch 31/2000\n",
      "766/766 [==============================] - 0s 106us/sample - loss: 1.3552e-05 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9792\n",
      "479/479 [==============================] - 0s 182us/sample - loss: 0.0988 - accuracy: 0.9812\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 525us/sample - loss: 1.4629 - accuracy: 0.5548 - val_loss: 0.5915 - val_accuracy: 0.7812\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 84us/sample - loss: 0.3915 - accuracy: 0.8708 - val_loss: 0.2757 - val_accuracy: 0.9271\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 76us/sample - loss: 0.1455 - accuracy: 0.9543 - val_loss: 0.1060 - val_accuracy: 0.9583\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 77us/sample - loss: 0.1205 - accuracy: 0.9621 - val_loss: 0.1307 - val_accuracy: 0.9323\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 74us/sample - loss: 0.0986 - accuracy: 0.9648 - val_loss: 0.1577 - val_accuracy: 0.9427\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 0.0300 - accuracy: 0.9961 - val_loss: 0.0826 - val_accuracy: 0.9688\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.1428 - val_accuracy: 0.9635\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 0.0128 - accuracy: 0.9974 - val_loss: 0.1175 - val_accuracy: 0.9688\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 77us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.9531\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 0.0159 - accuracy: 0.9935 - val_loss: 0.1135 - val_accuracy: 0.9740\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 0.0569 - accuracy: 0.9778 - val_loss: 0.2062 - val_accuracy: 0.9323\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0791 - accuracy: 0.9739 - val_loss: 0.1626 - val_accuracy: 0.9635\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 77us/sample - loss: 0.0634 - accuracy: 0.9830 - val_loss: 0.1772 - val_accuracy: 0.9531\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 66us/sample - loss: 0.0430 - accuracy: 0.9869 - val_loss: 0.1157 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0304 - accuracy: 0.9883 - val_loss: 0.1496 - val_accuracy: 0.9531\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 0.0139 - accuracy: 0.9961 - val_loss: 0.1640 - val_accuracy: 0.9688\n",
      "479/479 [==============================] - 0s 156us/sample - loss: 0.2454 - accuracy: 0.9415\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 0s 518us/sample - loss: 1.3366 - accuracy: 0.5535 - val_loss: 0.6607 - val_accuracy: 0.7500\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 91us/sample - loss: 0.3681 - accuracy: 0.8838 - val_loss: 0.3969 - val_accuracy: 0.8594\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 0.2046 - accuracy: 0.9308 - val_loss: 0.2117 - val_accuracy: 0.9062\n",
      "Epoch 4/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 74us/sample - loss: 0.1099 - accuracy: 0.9661 - val_loss: 0.1644 - val_accuracy: 0.9479\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 0.0989 - accuracy: 0.9648 - val_loss: 0.1587 - val_accuracy: 0.9427\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 0.0513 - accuracy: 0.9817 - val_loss: 0.0628 - val_accuracy: 0.9844\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 63us/sample - loss: 0.0217 - accuracy: 0.9948 - val_loss: 0.2395 - val_accuracy: 0.9219\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 76us/sample - loss: 0.0314 - accuracy: 0.9935 - val_loss: 0.1236 - val_accuracy: 0.9635\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 0.0588 - accuracy: 0.9778 - val_loss: 0.2129 - val_accuracy: 0.9115\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 0.0550 - accuracy: 0.9830 - val_loss: 0.0912 - val_accuracy: 0.9583\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 83us/sample - loss: 0.0357 - accuracy: 0.9935 - val_loss: 0.0917 - val_accuracy: 0.9740\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.1066 - val_accuracy: 0.9583\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 0.0134 - accuracy: 0.9948 - val_loss: 0.1628 - val_accuracy: 0.9375\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 0.0967 - accuracy: 0.9687 - val_loss: 0.2236 - val_accuracy: 0.9010\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 0.0433 - accuracy: 0.9843 - val_loss: 0.1124 - val_accuracy: 0.9583\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 85us/sample - loss: 0.0427 - accuracy: 0.9817 - val_loss: 0.0562 - val_accuracy: 0.9740\n",
      "Epoch 17/2000\n",
      "766/766 [==============================] - 0s 78us/sample - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.0667 - val_accuracy: 0.9792\n",
      "Epoch 18/2000\n",
      "766/766 [==============================] - 0s 81us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9740\n",
      "Epoch 19/2000\n",
      "766/766 [==============================] - 0s 87us/sample - loss: 4.6247e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9635\n",
      "Epoch 20/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 2.0671e-04 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9635\n",
      "Epoch 21/2000\n",
      "766/766 [==============================] - 0s 77us/sample - loss: 1.3270e-04 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 0.9635\n",
      "Epoch 22/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 1.0790e-04 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 0.9635\n",
      "Epoch 23/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 8.8136e-05 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9688\n",
      "Epoch 24/2000\n",
      "766/766 [==============================] - 0s 82us/sample - loss: 7.7660e-05 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9688\n",
      "Epoch 25/2000\n",
      "766/766 [==============================] - 0s 80us/sample - loss: 6.6389e-05 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9688\n",
      "Epoch 26/2000\n",
      "766/766 [==============================] - 0s 84us/sample - loss: 6.1739e-05 - accuracy: 1.0000 - val_loss: 0.0709 - val_accuracy: 0.9688\n",
      "479/479 [==============================] - 0s 166us/sample - loss: 0.1087 - accuracy: 0.9729\n",
      "Train on 766 samples, validate on 192 samples\n",
      "Epoch 1/2000\n",
      "766/766 [==============================] - 1s 732us/sample - loss: 1.2891 - accuracy: 0.6501 - val_loss: 0.5143 - val_accuracy: 0.8542\n",
      "Epoch 2/2000\n",
      "766/766 [==============================] - 0s 76us/sample - loss: 0.3839 - accuracy: 0.8708 - val_loss: 0.3640 - val_accuracy: 0.8698\n",
      "Epoch 3/2000\n",
      "766/766 [==============================] - 0s 72us/sample - loss: 0.2126 - accuracy: 0.9295 - val_loss: 0.3491 - val_accuracy: 0.8906\n",
      "Epoch 4/2000\n",
      "766/766 [==============================] - 0s 64us/sample - loss: 0.1676 - accuracy: 0.9386 - val_loss: 0.2987 - val_accuracy: 0.9062\n",
      "Epoch 5/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 0.0895 - accuracy: 0.9739 - val_loss: 0.2060 - val_accuracy: 0.9219\n",
      "Epoch 6/2000\n",
      "766/766 [==============================] - 0s 73us/sample - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.1157 - val_accuracy: 0.9479\n",
      "Epoch 7/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 0.0648 - accuracy: 0.9791 - val_loss: 0.3064 - val_accuracy: 0.9219\n",
      "Epoch 8/2000\n",
      "766/766 [==============================] - 0s 69us/sample - loss: 0.0448 - accuracy: 0.9869 - val_loss: 0.2603 - val_accuracy: 0.9479\n",
      "Epoch 9/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 0.0247 - accuracy: 0.9922 - val_loss: 0.2399 - val_accuracy: 0.9479\n",
      "Epoch 10/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 0.0296 - accuracy: 0.9896 - val_loss: 0.2053 - val_accuracy: 0.9323\n",
      "Epoch 11/2000\n",
      "766/766 [==============================] - 0s 72us/sample - loss: 0.0684 - accuracy: 0.9778 - val_loss: 0.2045 - val_accuracy: 0.9531\n",
      "Epoch 12/2000\n",
      "766/766 [==============================] - 0s 70us/sample - loss: 0.0302 - accuracy: 0.9883 - val_loss: 0.1817 - val_accuracy: 0.9531\n",
      "Epoch 13/2000\n",
      "766/766 [==============================] - 0s 65us/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9583\n",
      "Epoch 14/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9635\n",
      "Epoch 15/2000\n",
      "766/766 [==============================] - 0s 68us/sample - loss: 6.3383e-04 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9635\n",
      "Epoch 16/2000\n",
      "766/766 [==============================] - 0s 67us/sample - loss: 4.2123e-04 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9635\n",
      "479/479 [==============================] - 0s 173us/sample - loss: 0.0855 - accuracy: 0.9770\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x00000279BD945548>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-10726cd72cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m rnd_search_cv.fit(X_train, y_train, epochs=2000,\n\u001b[0;32m     12\u001b[0m                  \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                  callbacks=[EarlyStopping(patience=10)])\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[1;32m--> 762\u001b[1;33m                 **self.best_params_))\n\u001b[0m\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     96\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0;32m     97\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                                (estimator, name))\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x00000279BD945548>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [3,4,5,6,7,8],\n",
    "    \"n_neurons\": np.arange(100,800),\n",
    "    \"learning_rate\": reciprocal(3e-4,3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_clf, param_distribs, n_iter=10,cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=2000,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0014182873712938946, 'n_hidden': 5, 'n_neurons': 346}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9721642136573792"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-db070619cf2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model/digits/digits_best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n Accuracy: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model.save('./model/digits/digits_best_model.h5')\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, y_test, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
